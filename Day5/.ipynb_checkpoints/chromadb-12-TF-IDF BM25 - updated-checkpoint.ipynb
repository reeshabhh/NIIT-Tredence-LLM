{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f3ef8c-6ddb-41e1-876d-c565217780e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551101bc-18f8-4c62-b6c7-9c846c0856b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce586a4e-0b41-4efb-a19e-025f47ceb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb dataset\n",
    "file_path = r'D:\\AI-DATASETS\\02-MISC-large\\IMDB Dataset.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce6e427-2501-44bf-bd1b-4028336c8978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aebcd131-8d1a-4b2d-badd-2808dc3ececf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>I have seen so many bad reviews on Supervivien...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15070</th>\n",
       "      <td>A group of people are invited to there high sc...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32002</th>\n",
       "      <td>For anyone who has seen and fallen in love wit...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29739</th>\n",
       "      <td>I wasn't born until 4 years after this wonderf...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19638</th>\n",
       "      <td>Well, not yet, at least.&lt;br /&gt;&lt;br /&gt;It's not l...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27769</th>\n",
       "      <td>It is so nice to see Bruce Willis come down of...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>The comments for Commune make it sound like a ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32879</th>\n",
       "      <td>It is true that some fans of Peter Sellers wor...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11482</th>\n",
       "      <td>Christophe Lambert once said he was still maki...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8009</th>\n",
       "      <td>The thing that's truly terrifying about this i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "1991   I have seen so many bad reviews on Supervivien...  positive\n",
       "15070  A group of people are invited to there high sc...  negative\n",
       "32002  For anyone who has seen and fallen in love wit...  negative\n",
       "29739  I wasn't born until 4 years after this wonderf...  positive\n",
       "19638  Well, not yet, at least.<br /><br />It's not l...  negative\n",
       "27769  It is so nice to see Bruce Willis come down of...  positive\n",
       "8374   The comments for Commune make it sound like a ...  negative\n",
       "32879  It is true that some fans of Peter Sellers wor...  negative\n",
       "11482  Christophe Lambert once said he was still maki...  negative\n",
       "8009   The thing that's truly terrifying about this i...  negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185184c3-f540-46b2-983a-7b62d14830e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the first 1000 reviews\n",
    "reviews = df['review'].sample(1000).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a11f04-e4d2-486c-bb97-48add6cf0c9f",
   "metadata": {},
   "source": [
    "#### Generate TF-IDF Vectors\n",
    "- Use scikit-learn's TfidfVectorizer to generate TF-IDF vectors for the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe9d82b3-81ad-4d4f-9788-54cfe112cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a5cbb4-4d9c-4c66-a766-286db07e1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Limiting to 1000 features for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2021eaab-7192-4897-97ab-21a497e2f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TF-IDF vectors for the reviews\n",
    "tfidf_matrix = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac170db0-1ac8-4146-9f9f-1554f3879d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7cf9c-acf9-49d7-a5f9-9823627282f7",
   "metadata": {},
   "source": [
    "#### use chromaDB (vector database)\n",
    "- store the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f300d8de-c89f-4647-80bb-0b315f7400e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bab98c26-b2f9-4176-a6fb-1cd0214d160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d1034b-ea47-47f9-9096-ebf4958fb8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# List all collections\n",
    "collections = client.list_collections()\n",
    "print([collection.name for collection in collections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c5f648c-a8a6-414c-a723-92fae2782e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection to store TF-IDF vectors\n",
    "collection_name = client.get_or_create_collection(\"imdb_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec723a5-2846-433a-91f6-fcb2b9831add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81f961ce-2512-4d18-a5ec-1bb7658803b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TF-IDF matrix to dense array and insert into ChromaDB\n",
    "tfidf_dense = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c246f44e-7e9c-46b5-962d-1efa71d906ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26f51ee5-db83-493f-a023-63b39cefb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add each review vector into ChromaDB\n",
    "for idx, vector in enumerate(tfidf_dense):\n",
    "    collection_name.add(\n",
    "        ids       =[str(idx)],                  # Unique ID for each review\n",
    "        embeddings=[vector],                    # The TF-IDF vector\n",
    "        metadatas =[{\"review\": reviews[idx]}],  # Store the actual review\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b18658-a2fe-489b-9a58-32af576957db",
   "metadata": {},
   "source": [
    "- ids: Unique identifier for each review.\n",
    "- embeddings: The TF-IDF vectors.\n",
    "- metadatas: Metadata like the actual review text, which will be retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31119f34-b41b-420b-933a-3129dc6c271c",
   "metadata": {},
   "source": [
    "#### Querying ChromaDB with TF-IDF\n",
    "- query ChromaDB to retrieve similar reviews using a TF-IDF-based retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff466c8d-0a9d-4aef-b3e4-0e248d14a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(query_text, top_k=5):\n",
    "    # Convert the query to a TF-IDF vector\n",
    "    query_vector = vectorizer.transform([query_text]).toarray()[0]\n",
    "    \n",
    "    # Perform similarity search in ChromaDB\n",
    "    results = collection_name.query(\n",
    "        query_embeddings=[query_vector],  # The query vector\n",
    "        n_results       =top_k  # Number of results to return\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "329ee73a-0be0-4a12-b399-18674d43baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query_text = \"I love movies about space adventures\"\n",
    "result = query_chromadb(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a950992c-3b20-4cb5-9cfe-d02153fb882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f221cd9-729d-4218-8463-7483003cd821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'distances', 'metadatas', 'embeddings', 'documents', 'uris', 'data', 'included'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96158c41-a4d5-4f49-8d71-a0bce7db1a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.7007890939712524,\n",
       "  1.7408870458602905,\n",
       "  1.7800662517547607,\n",
       "  1.784860372543335,\n",
       "  1.792397141456604]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['distances']             # L2 based norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b350691-4b07-48fa-8b62-3eb226956191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First of all, even IMDb is slacking with this movie, as the list of cast is VERY \"gappy\". Even main characters are missing from it like Buddy for example.<br /><br />Now back to the movie. I love watching movies, but this one tortured me throughout the whole 82 or however many minutes. It was HORRID. Probably the worst movie I have ever seen. And the reason why it bothers me so much, is because I was quite excited about seeing it beforehand.<br /><br />The plot line itself is good. It could have been a great film if done properly and with careful casting. Golden Brooks was a HUGE disappointment. I now see that the only role she can be good in is the loud, fun-loving, energetic sexy chick she plays on Girlfriends. Melodramatic roles are not for her at all. She basically killed her character, Rachel.<br /><br />I love some of the cast, like Deborah Cox, Mel Jackson and Darrin Dewitt Henson, but even they couldn't save this catastrophic movie. Of course it is only my personal opinion that I can not stand Hill Harper as a man, and he didn't help me get to like him more here either.<br /><br />Golden Brooks' voice bothered me so much that I actually had to force myself to finish watching the movie. It seemed like she whispered throughout the whole joint. The director I won't even waste board-space on, he did such a bad job. The editing, photography/camera focus and just about everything about this movie was SAD. Not to mention the dialogs! Absolutely unreal and many times straight hilarious (when it was supposed to make you cry and search deep within yourself). <br /><br />As I said, it could have been a very nice movie, but it was seriously messed up. I would NOT recommend it to anyone, unless they are cinematography major and want to see a 'What not to do' example for their future work.<br /><br />Have a great day, Everyone!!!\n",
      "---\n",
      "from the view of a NASCAR Maniac like I am, the movie is interesting. You can see many race cars from 1983. Even tough, the racing scenes are not that much realistic. But I have to admit, that I haven't seen any race before 1995, because before that time, they didn't show any NASCAR races in Germany)<br /><br />from the view of a Burt Reynolds fan like I am, the movie basically is what we are used to see from Reynolds in the 80's: Burt behind the wheel of a fast car, like in his Bandit Movies.<br /><br />If you love NASCAR and Burt Reynolds, this movie is a must-see. If you only love one of this 2 things, I also recommend to watch it. If you like neither NASCAR nor Burt Reynolds, you still should give it a chance, but remember, this movie was far away from winning an Oscar Academy Award.<br /><br />It is the typical humor of the 80's. If you like movies like the Cannonball Movies, and Police Academy, you will also like that one.\n",
      "---\n",
      "Basically, take the concept of every Asian horror ghost movie and smash it into one and you get this movie. The story goes like this: a bunch of college kids get voice mails from their own phones that are foretelling their deaths. There's some s*** going on with ghosts, which if you've seen any Asian ghost movie, isn't scary by now. This movie was quite upsetting because it's very clichéd. It's the same bullcrap, different movie.<br /><br />The acting was pretty good. Unfortunately the actors are put into a very Ring-esquire situation, so it's nothing we haven't seen in the past. The two lead acts did a solid job though.<br /><br />As far as gore, there's not much going on. We get a cool sequence that includes an arm twisting a head off (I don't know how else to explain that), but it was cut away so you don't see anything except the final result. You see some blood at times, including decapitated arms and a zombie (that looked really cool I might add), but this movie isn't too bloody.<br /><br />The scares in the movie are few and spread out, and it's really not that scary. You'll get some creepy images at times, but it's not enough for me to consider scary. It's nothing different from Ringu, Ju-On, or Dark Water, and none of those scared me either. That's really the downfall of this (and most Asian horror movies) is that if it doesn't deliver the scares then it's just not that good.<br /><br />As far as directing, Takashi Miike still did a pretty good job. He seemed a little tamed in this movie compared to his past movies, but he still portrays a lot of his messed up style he's become famous for. A lot of images were a lot like Miike (including a scene with a bunch of jars of dead fetuses), and the last 15 - 20 minutes seemed far more Miike then the rest of the movie. Still, the movie is flawed by its unoriginality.<br /><br />I would recommend this only to people who are huge on Asian horror movies (even if you are, I can recommend much better) or big Miike fans. Warning to those who want to get into Miike, this is NOT his best work.<br /><br />I'm giving it a 4 because it's just mediocre. Perhaps if this was released 4 or 5 years ago it might be worth a higher rating.<br /><br />Also, I'd like to b**** about Asian horror movies real quick. How come if it's an Asian horror movie it's automatically suppose to be good over here (US)? A LOT of these movies are the equivalent in Japan to what Scream, Urban Legends, and I Know What You Did Last Summer were over here in the 90's. If you've seen one you've seen them all. And a lot of these movies rely way too much on scares and imagery that if it doesn't deliver the scares they set out to do then they're just not that good, and nothing would change that. More Asian horror films need to be more like Audition and A Tale of Two Sisters, two movies that if they don't frighten or scare you, at least they have great stories, acting, direction, cinematography, and much more to back them up. Two movies that aren't just great horror movies, but great movies in general. More Asian horror movies need to be like these instead of the cliché, \"A ghost just wanted to be found so it went around killing people through their phone/video tape/house/electric appliance/water pipes/google search engine/vibrator/groceries/etc.\"\n",
      "---\n",
      "I don't cry easily over movies, but I have to admit, this one brought me to tears. Although I am not a Ms. Streep fan, her performance was excellent. The title defines in a sentence what a mother's love is. For the first hour I didn't like any of the characters, but that changed as the movie went on. The movie also explained why certain marriages last even though there are obstacles. A must see film.\n",
      "---\n",
      "\"Idiocracy\" is the latest film to come from Mike \"Office Space\" Judge, and it certainly follows a similar theme of that film in the fact that it is an observation of stupidity and how mediocrity can overcome adversity... relatively speaking. It is a story about Joe Bauer (Luke Wilson), who is, quite literally, the most average guy in existence. Joe, and a prostitute named Rita (Maya Rudolph), become the test subjects for a military project of a hibernation chamber. They were to remain suspended for only one year, but due to lack of oversight, Joe and Rita are forgotten about and accidentally wake up 500 years in the future.<br /><br />Here's the scary part: This film explains, in a very realistic and plausible way, how the entire population of 2505 became absolutely retarded. With no natural predators, the evolution of the human species does not necessarily favor the quickest, smartest, and strongest people for progression of genes... just the people who breed the most. Unfortunately, those people happen to be welfare-sucking, trailer trash idiots who breed like rabbits. This abundant reproduction of the stupid people has caused an adverse effect on societal growth and now Joe and Rita are the two smartest human beings on the face of the planet. If it helps, imagine the entire population as just a hybrid of rednecks, jocks, cholos and hoochies. Seeing this nightmarish dystopia, Joe learns of and attempts to track down a time machine to see if he and Rita can get back to when they came from, and that's basically the whole plot.<br /><br />But despite how one-dimensional I may make it sound, this movie is higher brow than you can fathom. Nuances are everywhere and anyone can see glimpses (warning signs, if you will) of modern day dumb-ciety permeating facets of everyday life and turning it into the train wreck on display in \"Idiocracy.\" The film has some truly awesome showcases of realistic retardedness put on a pedestal. I don't want to give anything away and ruin jokes for you, but let's just say that it is pretty thorough. I can see how some would say that it is just a lot of toilet humor, but it, odd as it may seem, has a purpose; to show how dumb and crass these people are.<br /><br />This film, unfortunately, is destined to see the same fate as its predecessor, \"Office Space\"; no one will see it in theaters, but everyone will brag about discovering this awesome/funny movie when it comes out on video. My only complaint for the film would be that the flow of the narrative sometimes gets broken so they can do a Hitchhiker's-Guide-to-the-Galaxy type exposition on how things got to be where they are, but it is a necessary evil and is implemented better here. Other than that, good characters, funny jokes, and better-than-average social commentary wrapped up in a funny bow.<br /><br />Final Note: If seeing our youth becoming gang-banger wanna-be's, acting like redneck/ ghetto trash and being proud of it... if you are educated and cultured in anyway and can see how our country is spiraling out of control into an abyss of stupidity, for god sakes, watch this movie.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for idx, review in enumerate(result['metadatas'][0]):\n",
    "    print(review['review'])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11e170-7c3e-404b-85b3-c5f5f817bcec",
   "metadata": {},
   "source": [
    "#### books.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12aefd56-22fe-41f4-825d-db49f910812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the books csv\n",
    "# https://www.kaggle.com/datasets/saurabhbagchi/books-dataset/data\n",
    "file_path = r'D:\\AI-DATASETS\\02-MISC-large\\books.csv'\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1', sep=';', on_bad_lines='skip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d3a06a6-ab83-41ad-9736-d2494d455640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271360, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "022bbe53-2183-4e3d-996b-8e422cd1e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Image-URL-S', 'Image-URL-M',\t'Image-URL-L'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "619d75d3-2f21-49cd-b0b4-75b2a7fdf09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105537</th>\n",
       "      <td>0451188144</td>\n",
       "      <td>Dr. Nightingale Meets Puss in Boots: A Deirdre...</td>\n",
       "      <td>Lydia Adamson</td>\n",
       "      <td>1997</td>\n",
       "      <td>Signet Book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116568</th>\n",
       "      <td>0380704846</td>\n",
       "      <td>Rosemoore</td>\n",
       "      <td>Arthur Cavanaugh</td>\n",
       "      <td>1989</td>\n",
       "      <td>Harper Mass Market Paperbacks (Mm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205993</th>\n",
       "      <td>080501196X</td>\n",
       "      <td>Oh, Fudge: A Celebration of America's Favorite...</td>\n",
       "      <td>Lee Edwards Benning</td>\n",
       "      <td>1990</td>\n",
       "      <td>Henry Holt &amp;amp; Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237702</th>\n",
       "      <td>0440404150</td>\n",
       "      <td>The Princess of the Fillmore Street School</td>\n",
       "      <td>Marjorie Weinman Sharmat</td>\n",
       "      <td>1991</td>\n",
       "      <td>Yearling Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259136</th>\n",
       "      <td>0451178696</td>\n",
       "      <td>The Willful Widow (Signet Regency Romance)</td>\n",
       "      <td>Evelyn Richardson</td>\n",
       "      <td>1994</td>\n",
       "      <td>New Amer Library (Mm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110829</th>\n",
       "      <td>0843925035</td>\n",
       "      <td>Promises to Keep</td>\n",
       "      <td>Wendy Susans</td>\n",
       "      <td>1987</td>\n",
       "      <td>Leisure Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35800</th>\n",
       "      <td>0751526568</td>\n",
       "      <td>Smithy</td>\n",
       "      <td>Ian Mackersey</td>\n",
       "      <td>2000</td>\n",
       "      <td>Trafalgar Square Publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178023</th>\n",
       "      <td>0671874888</td>\n",
       "      <td>Goodbye, Janette</td>\n",
       "      <td>Harold Robbins</td>\n",
       "      <td>1997</td>\n",
       "      <td>Pocket Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25021</th>\n",
       "      <td>0671526812</td>\n",
       "      <td>HIGH STAKES HR PRE (Harold Robbins Presents Se...</td>\n",
       "      <td>John Fischer</td>\n",
       "      <td>1986</td>\n",
       "      <td>Pocket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124637</th>\n",
       "      <td>0373095538</td>\n",
       "      <td>The Welcoming (Silhouette Special Edition No. ...</td>\n",
       "      <td>Nora Roberts</td>\n",
       "      <td>1989</td>\n",
       "      <td>Silhouette</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN                                         Book-Title  \\\n",
       "105537  0451188144  Dr. Nightingale Meets Puss in Boots: A Deirdre...   \n",
       "116568  0380704846                                          Rosemoore   \n",
       "205993  080501196X  Oh, Fudge: A Celebration of America's Favorite...   \n",
       "237702  0440404150         The Princess of the Fillmore Street School   \n",
       "259136  0451178696         The Willful Widow (Signet Regency Romance)   \n",
       "110829  0843925035                                   Promises to Keep   \n",
       "35800   0751526568                                             Smithy   \n",
       "178023  0671874888                                   Goodbye, Janette   \n",
       "25021   0671526812  HIGH STAKES HR PRE (Harold Robbins Presents Se...   \n",
       "124637  0373095538  The Welcoming (Silhouette Special Edition No. ...   \n",
       "\n",
       "                     Book-Author Year-Of-Publication  \\\n",
       "105537             Lydia Adamson                1997   \n",
       "116568          Arthur Cavanaugh                1989   \n",
       "205993       Lee Edwards Benning                1990   \n",
       "237702  Marjorie Weinman Sharmat                1991   \n",
       "259136         Evelyn Richardson                1994   \n",
       "110829              Wendy Susans                1987   \n",
       "35800              Ian Mackersey                2000   \n",
       "178023            Harold Robbins                1997   \n",
       "25021               John Fischer                1986   \n",
       "124637              Nora Roberts                1989   \n",
       "\n",
       "                                 Publisher  \n",
       "105537                         Signet Book  \n",
       "116568  Harper Mass Market Paperbacks (Mm)  \n",
       "205993                 Henry Holt &amp; Co  \n",
       "237702                      Yearling Books  \n",
       "259136               New Amer Library (Mm)  \n",
       "110829                       Leisure Books  \n",
       "35800          Trafalgar Square Publishing  \n",
       "178023                        Pocket Books  \n",
       "25021                               Pocket  \n",
       "124637                          Silhouette  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "603465e2-5565-427e-bc2c-3931b20261a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,re\n",
    "#import spacy\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db76516e-953a-45a6-8279-8bcc93b88441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()  # Lowercasing\n",
    "    \n",
    "    # Remove all punctuation except '&'\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation.replace('&', '')))\n",
    "    \n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove excessive whitespace using regex\n",
    "    #print(f\"Without excessive whitespace: {text}\")\n",
    "    \n",
    "    # Normalize &amp; if it exists\n",
    "    text = re.sub(r'&amp;', 'and', text)\n",
    "    #print(f\"After replacing '&amp;': {text}\")\n",
    "    \n",
    "    # Replace any remaining & with 'and'\n",
    "    text = text.replace('&', 'and')\n",
    "    #print(f\"After replacing '&': {text}\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45c700cf-5078-427e-ad0c-488ded0650c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example with excessive and whitespace\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sample_text = \"This is an example   with   excessive  & whitespace!\"\n",
    "cleaned_text = preprocess_text(sample_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d552146-293d-4f6e-a5b1-a7916c9e5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any null values\n",
    "df_cleaned = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e663e5fe-1d73-4a26-9ac8-af696df5f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.42 s\n",
      "Wall time: 4.23 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Apply the preprocessing\n",
    "df_cleaned['text'] = df_cleaned['Book-Title'] + ' ' + df_cleaned['Book-Author'] + ' ' + df_cleaned['Publisher']\n",
    "df_cleaned['text'] = df_cleaned['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9457ddca-1873-4e33-b867-03a6c75b6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_samples = df_cleaned.sample(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f51cbe0-ac28-48f3-a0a5-ec152c0b7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8906558d-8106-4107-9c98-5ed6fdd029ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate TF-IDF vectors for the reviews\n",
    "tfidf_matrix = vectorizer.fit_transform(df_cleaned_samples.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed2e96ca-1763-450a-a77f-b8c13657f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ae8224d-e4b8-4a77-baa7-b002cd89166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imdb_reviews']\n"
     ]
    }
   ],
   "source": [
    "# List all collections\n",
    "collections = client.list_collections()\n",
    "print([collection.name for collection in collections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3dbb209a-32d6-4c25-a46c-b8ca3c2d4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TF-IDF matrix to dense array and insert into ChromaDB\n",
    "tfidf_dense = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8f595d7-6d95-43c2-96ea-4b631c2a48a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidDimensionException",
     "evalue": "Embedding dimension 8717 does not match collection dimensionality 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidDimensionException\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:19\u001b[0m\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:85\u001b[0m, in \u001b[0;36mCollection.add\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m    ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m (\n\u001b[0;32m     76\u001b[0m     ids,\n\u001b[0;32m     77\u001b[0m     embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m     ids, embeddings, metadatas, documents, images, uris\n\u001b[0;32m     83\u001b[0m )\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_add(ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, embeddings, metadatas, documents, uris)\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\site-packages\\chromadb\\api\\segment.py:380\u001b[0m, in \u001b[0;36mSegmentAPI._add\u001b[1;34m(self, ids, collection_id, embeddings, metadatas, documents, uris)\u001b[0m\n\u001b[0;32m    366\u001b[0m validate_batch(\n\u001b[0;32m    367\u001b[0m     (ids, embeddings, metadatas, documents, uris),\n\u001b[0;32m    368\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_max_batch_size()},\n\u001b[0;32m    369\u001b[0m )\n\u001b[0;32m    370\u001b[0m records_to_submit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    371\u001b[0m     _records(\n\u001b[0;32m    372\u001b[0m         t\u001b[38;5;241m.\u001b[39mOperation\u001b[38;5;241m.\u001b[39mADD,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m     )\n\u001b[0;32m    379\u001b[0m )\n\u001b[1;32m--> 380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_embedding_record_set(coll, records_to_submit)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_producer\u001b[38;5;241m.\u001b[39msubmit_embeddings(collection_id, records_to_submit)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_product_telemetry_client\u001b[38;5;241m.\u001b[39mcapture(\n\u001b[0;32m    384\u001b[0m     CollectionAddEvent(\n\u001b[0;32m    385\u001b[0m         collection_uuid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     )\n\u001b[0;32m    391\u001b[0m )\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\site-packages\\chromadb\\api\\segment.py:883\u001b[0m, in \u001b[0;36mSegmentAPI._validate_embedding_record_set\u001b[1;34m(self, collection, records)\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m record[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dimension(\n\u001b[0;32m    884\u001b[0m             collection, \u001b[38;5;28mlen\u001b[39m(record[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]), update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    885\u001b[0m         )\n",
      "File \u001b[1;32mD:\\ANACONDA\\Lib\\site-packages\\chromadb\\api\\segment.py:899\u001b[0m, in \u001b[0;36mSegmentAPI._validate_dimension\u001b[1;34m(self, collection, dim, update)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sysdb\u001b[38;5;241m.\u001b[39mupdate_collection(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m, dimension\u001b[38;5;241m=\u001b[39mdim)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m collection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidDimensionException(\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match collection dimensionality \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m     )\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mInvalidDimensionException\u001b[0m: Embedding dimension 8717 does not match collection dimensionality 1000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Store the original indices before any resetting\n",
    "original_indices = df_cleaned_samples.index.tolist()  # Store original indices\n",
    "\n",
    "# Add each review vector into ChromaDB\n",
    "for idx, vector in enumerate(tfidf_dense):\n",
    "\n",
    "    # Use original_indices to fetch metadata\n",
    "    original_idx = original_indices[idx]\n",
    "    \n",
    "    # Construct metadata with book information\n",
    "    metadata = {\n",
    "        \"Title\": df_cleaned_samples.loc[original_idx, 'Book-Title'],          # Book Title\n",
    "        \"Author\": df_cleaned_samples.loc[original_idx, 'Book-Author'],        # Book Author\n",
    "        \"Year\": df_cleaned_samples.loc[original_idx, 'Year-Of-Publication'],  # Year of Publication\n",
    "        \"Publisher\": df_cleaned_samples.loc[original_idx, 'Publisher'],       # Publisher\n",
    "    }\n",
    "    \n",
    "    # Add the vector and metadata to ChromaDB collection\n",
    "    collection_name.add(\n",
    "        ids=[str(idx)],                  # Unique ID for each book/review\n",
    "        embeddings=[vector],             # The TF-IDF vector\n",
    "        metadatas=[metadata]             # Store book details (metadata)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66e4e933-3f68-4daf-b7f4-a7d9aaa9b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(query_text, top_k=3):\n",
    "    # Convert the query to a TF-IDF vector\n",
    "    query_vector = vectorizer.transform([query_text]).toarray()[0]\n",
    "    \n",
    "    # Perform similarity search in ChromaDB\n",
    "    results = collection_name.query(\n",
    "        query_embeddings=[query_vector],  # The query vector\n",
    "        n_results       =top_k            # Number of results to return\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a93dc1-8236-4452-82f2-13f4efb47ef9",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55387aca-9b86-411c-a9ca-9fbf6d2e6fb9",
   "metadata": {},
   "source": [
    "BM25 (Best Matching 25) is a ranking function used in information retrieval systems to evaluate the relevance of documents in relation to a query. It builds upon the probabilistic information retrieval model and is particularly effective for scoring and ranking documents based on the frequency of query terms within them. Here's a breakdown of BM25 and its differences from TF-IDF:\n",
    "\n",
    "**Relevance Scoring:** BM25 calculates a score for each document based on the presence and frequency of the terms in the query. The score reflects how well the document matches the query.\n",
    "\n",
    "**Components:**\n",
    "\n",
    "- **Term Frequency (TF):** Similar to TF-IDF, BM25 considers how often a term appears in a document. However, it uses a saturation function to diminish the effect of term frequency as it increases.\n",
    "  \n",
    "- **Inverse Document Frequency (IDF):** BM25 employs IDF to account for the rarity of terms. Rare terms contribute more to the score than common terms.\n",
    "  \n",
    "- **Document Length Normalization:** BM25 normalizes for document length, ensuring that longer documents do not have an unfair advantage simply because they contain more terms.\n",
    "  \n",
    "- **Parameters:** BM25 has parameters like `k1` (controls the impact of term frequency) and `b` (adjusts the normalization based on document length). This flexibility allows for tuning based on specific datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fad048-ee03-4766-a5c2-621ad02a5740",
   "metadata": {},
   "source": [
    "##### Why We Need to Saturate the Term Frequency (TF)\n",
    "\n",
    "##### 1. Diminishing Returns on Term Relevance\n",
    "- As the frequency of a term in a document increases, its contribution to relevance does not increase linearly. In fact, the impact of additional occurrences of the term diminishes.\n",
    "- For instance, if a document mentions a keyword 1 time, it may be relevant; if it mentions it 10 times, it doesn't necessarily mean it is 10 times more relevant. Saturation accounts for this diminishing returns effect.\n",
    "\n",
    "##### 2. Avoiding Overemphasis on Frequent Terms\n",
    "- Without saturation, documents with very high term frequencies might be unfairly prioritized, even if the actual content and relevance to the query are low.\n",
    "- This is particularly important for documents that may repeat a term excessively, as they could skew the results and lead to poor search quality.\n",
    "\n",
    "##### 3. Enhancing Precision in Ranking\n",
    "- Saturation helps create a more balanced scoring system. It allows for a better distinction between documents with varying term frequencies.\n",
    "- For example, a document with a term frequency of 5 might be seen as more relevant than one with a frequency of 1, but not overwhelmingly so. The use of saturation can help refine the score, ensuring the ranking is more precise and meaningful.\n",
    "\n",
    "##### 4. Consistency Across Document Lengths\n",
    "- Different documents can vary significantly in length, leading to variations in raw term frequencies. Saturation helps normalize these differences.\n",
    "- This is particularly important for long documents where high term frequency might be a result of sheer length rather than actual relevance.\n",
    "\n",
    "##### 5. Parameter Control\n",
    "- Saturation provides a means to control the behavior of the scoring function through parameters like `k1`. By adjusting this parameter, users can fine-tune how quickly the effects of term frequency diminish, allowing for flexibility based on specific datasets and requirements.\n",
    "\n",
    "##### Example of Term Frequency Saturation\n",
    "Consider a term \"machine learning\" in two documents:\n",
    "\n",
    "- **Document A:** \"Machine learning is a fascinating field. Machine learning can change the world.\"\n",
    "- **Document B:** \"Machine learning is machine learning machine learning machine learning machine learning machine learning.\"\n",
    "\n",
    "- **Without Saturation:** Document B might score much higher due to the raw count of term occurrences (5 times).\n",
    "- **With Saturation:** Document A might still score higher or similarly due to its context and meaningful use of the term, even though it appears less frequently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6656834-f611-4102-990a-391c0f7fb15e",
   "metadata": {},
   "source": [
    "#### 2. Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency (IDF) in BM25 is conceptually similar to IDF in TF-IDF, but there are some differences in how they are calculated and used in their respective formulas. Here’s a breakdown of the similarities and differences:\n",
    "\n",
    "`Similarities`\n",
    "\n",
    "`Purpose`: Both IDF measures are designed to reflect the importance of a term across a collection of documents. The primary goal is to reduce the weight of common terms and increase the weight of rare terms in the scoring process.\n",
    "\n",
    "`Concept`: In both cases, IDF is based on the idea that terms that appear in many documents are less informative than terms that appear in fewer documents. Therefore, IDF contributes to emphasizing the significance of rarer terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ace91-8140-4d37-bc37-9df9b4b2dcbf",
   "metadata": {},
   "source": [
    "`Differences`\n",
    "\n",
    "1. Mathematical Formulation:\n",
    "   \n",
    "- `TF-IDF IDF`:\n",
    "\n",
    "$$\n",
    "\\operatorname{IDF}(t)=\\log \\left(\\frac{N}{\\operatorname{df}(t)}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Where $N$ is the total number of documents and $\\operatorname{df}(t)$ is the number of documents containing the term $t$.\n",
    "\n",
    "- `BM25 IDF`:\n",
    "\n",
    "$$\n",
    "\\operatorname{IDF}(t)=\\log \\left(\\frac{N-\\operatorname{df}(t)+0.5}{\\operatorname{df}(t)+0.5}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "In BM25, a smoothing factor (0.5) is added to both the numerator and denominator to prevent division by zero and to smooth the effect of terms that appear in very few documents.\n",
    "\n",
    "2. Normalization:\n",
    "   \n",
    "- BM25 applies a more nuanced form of normalization, which makes the IDF component more robust in cases where terms are either very common or very rare. The added constants help avoid extreme values, making the model more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4038b3-be95-4131-b5d2-a0212fdedab3",
   "metadata": {},
   "source": [
    "The IDF in BM25 is calculated as the difference between the total number of documents \\( N \\) and the document frequency. This formulation introduces a focus on both the `rarity` of the term and its `relative presence` in the collection.\n",
    "\n",
    "##### Intuition:\n",
    "\n",
    "###### 1. Balance of Commonality and Rarity:\n",
    "By using ( N - $\\text{df}(t) $), BM25 considers how many documents do not contain the term, enhancing the importance of rare terms even further. If a term appears in very few documents, this number is high, which leads to a higher IDF score.\n",
    "\n",
    "###### 2. Proportional Contribution:\n",
    "The division by df(t) ensures that the IDF is normalized against the term's frequency. A term present in a few documents but still common in those documents will receive a lower weight than a term that is both rare and non-trivial in its presence.\n",
    "\n",
    "###### 3. Saturation Effect:\n",
    "This formulation captures the idea of term frequency saturation better, where the IDF score accounts for diminishing returns as the document frequency increases. As more documents contain the term, its ability to distinguish documents decreases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22f1b8-1fe6-41b6-853c-9894cd29a86e",
   "metadata": {},
   "source": [
    "#### 3. Document Length Normalization in BM25\n",
    "\n",
    "BM25 incorporates document length normalization to ensure that longer documents do not have an unfair advantage in the scoring process simply because they contain more terms. \n",
    "\n",
    "`How Normalization is Achieved:`\n",
    "1. **Length Parameters**: BM25 uses a parameter \\( b \\) (typically set between 0 and 1) to control the degree of normalization. A value of \\( b = 1 \\) applies full normalization, while \\( b = 0 \\) means no normalization.\n",
    "2. **Length Calculation**: The document length is measured in terms of the total number of terms. BM25 compares this length against the average document length in the collection.\n",
    "3. **Score Adjustment**: The normalization is applied during the score calculation. It adjusts the term frequency based on the length of the document relative to the average length, reducing the score for longer documents while increasing it for shorter ones.\n",
    "\n",
    "#### Example:\n",
    "- **Document A** (100 words) contains the term \"AI\" 10 times.\n",
    "- **Document B** (200 words) also contains the term \"AI\" 20 times.\n",
    "\n",
    "Without normalization, Document B would have a higher score due to higher term frequency. However, BM25 adjusts for this by taking into account the document lengths, ensuring that Document A's relevance is appropriately recognized despite its shorter length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c87466-6a12-4614-80af-2675a083f5d8",
   "metadata": {},
   "source": [
    "#### BM25 Formula\n",
    "The BM25 scoring function can be represented as follows:\n",
    "\n",
    "$$\n",
    "\\operatorname{BM} 25(d, q)=\\sum_{i=1}^{|q|} I D F\\left(t_i\\right) \\cdot \\frac{T F\\left(t_i, d\\right) \\cdot\\left(k_1+1\\right)}{T F\\left(t_i, d\\right)+k_1 \\cdot\\left(1-b+b \\cdot \\frac{|d|}{\\text { avgdl }}\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $d=$ document\n",
    "- $q=$ query\n",
    "- $t_i=$ term in the query\n",
    "- $T F\\left(t_i, d\\right)=$ term frequency of $t_i$ in document $d$\n",
    "- $|d|=$ length of the document (number of terms)\n",
    "- $\\operatorname{avgdl}=$ average document length across the corpus\n",
    "- $I D F\\left(t_i\\right)=$ inverse document frequency of term $t_i$\n",
    "- $k_1$ and $b=$ tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79567473-2a41-437f-a5c0-0f34867bab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acddb08b-6264-4ef8-875b-84cb01e37b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1479343-12ab-49ce-b3fa-eaf656643d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents (preprocessed as tokenized lists of words)\n",
    "corpus = [\n",
    "    \"The cat in the hat\",\n",
    "    \"The quick brown fox\",\n",
    "    \"The lazy dog sleeps\",\n",
    "    \"Fox is a wild animal\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4b58da6-c9a8-44ed-a818-0c7a77373429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each document in the corpus\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7876dfd-50a9-46ec-9922-b7c40268de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cece519-843a-4b39-a448-5f23d9212936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = word_tokenize(\"fox wild\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a765a64-af04-474a-b229-5022c29efad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BM25 scores\n",
    "scores = bm25.get_scores(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10abb9b5-ab80-44e5-ba50-623bb519658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.80695034]\n"
     ]
    }
   ],
   "source": [
    "# Output the scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d072a4b-1109-4505-a095-4614af17abf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
