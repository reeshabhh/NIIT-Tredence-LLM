{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb2c1a7-1b07-4121-bc9f-4dd22e19dd74",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "#### Prompt Engg \n",
    "\n",
    "- basic example using `chat completions`\n",
    "- code contains exmaples from 0.28 and later 1.35/1.45 versions of API\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9472ac92-ddeb-4f35-973d-f1679fdb6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613ddbe5-896c-4ef6-9df6-218d8c49b6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.45.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2235e69d-0c18-4a74-af6c-d200286f8e6f",
   "metadata": {},
   "source": [
    "#### models in openAI\n",
    "------------------------------------\n",
    "\n",
    "OpenAI provides a variety of models, each suited for different use cases, ranging from text generation, code generation, to specific tasks like question answering, summarization, and more.\n",
    "\n",
    "| Model Type               | Description                                                                                       | Model ID               | Variants/Use Cases                                           |\n",
    "|--------------------------|---------------------------------------------------------------------------------------------------|------------------------|-------------------------------------------------------------|\n",
    "| **GPT-4 Models**         | The most advanced language model, designed for a wide range of tasks, including text completion and complex reasoning. | `gpt-4`                | Variants: `gpt-4`, `gpt-4-32k` (larger context window)     |\n",
    "| **GPT-3.5 Models**       | A slightly earlier version of the GPT-4 model, used for efficient and high-quality completions across various tasks. | `gpt-3.5-turbo`        |                                                             |\n",
    "| **Codex Models**         | Specialized for code generation tasks, including programming in multiple languages.              | `code-davinci-002`, `code-cushman-001` | Use cases: Autocomplete for code, bug fixes, code explanations. |\n",
    "| **Text-Davinci Models**  | One of the most capable models for text generation, document completion, and more.              | `text-davinci-003`     | Use cases: Writing, summarization, text generation.         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee2693-9245-44bd-920c-79cb562dd03e",
   "metadata": {},
   "source": [
    "#### Endpoints\n",
    "-----------------------------\n",
    "\n",
    "- OpenAI provides several API endpoints that enable developers to interact with their models for various tasks such as `text generation`, `conversation`, `code completion`, `embedding generation`, and more.\n",
    "\n",
    "- Each endpoint serves a different purpose and can be accessed via HTTP requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521305b9-0b61-4dfc-bff6-42bfe3099789",
   "metadata": {},
   "source": [
    "#### 1. Text Completion (Legacy) Endpoint (`/v1/completions`)\n",
    "\n",
    "While still technically supported, the Completions endpoint is considered legacy. It directly generates text from a prompt without the conversation history format used in the Chat endpoint.\n",
    "\n",
    "**API Endpoint**:  \n",
    "POST https://api.openai.com/v1/completions\n",
    "\n",
    "**Usage**:  \n",
    "Legacy usage for tasks like document completion, content generation, and summarization.\n",
    "\n",
    "**Key Parameters**:\n",
    "- `model`: Specifies the model (e.g., `text-davinci-003`).\n",
    "- `prompt`: The input text to be completed.\n",
    "- `max_tokens`: The number of tokens to generate.\n",
    "- `temperature`: Controls randomness in the output (higher value = more creative, lower = more deterministic).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4640b6-2e95-40ef-ae6f-9ea9b026096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ca697-4d14-4900-9f8f-dd9609946bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for older 0.28 API\n",
    "# Make a request to the Completions Endpoint\n",
    "# response = openai.Completion.create(\n",
    "#   model       = \"gpt-3.5-turbo\",                                # Specify the model you want to use\n",
    "#   prompt      = \"Once upon a time, there was a wise owl that\",  # Input prompt\n",
    "#   max_tokens  = 50,                                             # The maximum number of tokens to generate\n",
    "#   temperature = 0.7,                                            # Controls randomness (0.7 = moderate creativity)\n",
    "#   n           = 1,                                              # The number of completions to generate\n",
    "#   stop        = None                                            # Optional stop sequence\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e6ff7-2758-4e80-a23b-625058b228f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the generated completion\n",
    "# print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db6024-e0c4-4198-baed-3c9de7b5e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "#     messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     response = openai.ChatCompletion.create(                            # older 0.28 \n",
    "#         model      = model,\n",
    "#         messages   = messages,\n",
    "#         temperature= 0, # this is the degree of randomness of the model's output\n",
    "#     )\n",
    "#     return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6381a-4b9c-468f-a05c-4cdcf341b58f",
   "metadata": {},
   "source": [
    "\n",
    "**Usage**: It’s the recommended endpoint for most tasks, including text completion, Q&A, summarization, etc., by providing a chat history as input.\n",
    "\n",
    "**Key Parameters**:\n",
    "\n",
    "- **model**: Specifies the model to be used (e.g., gpt-4 or gpt-3.5-turbo).\n",
    "- **messages**: A list of messages, where each message contains:\n",
    "  - **role**: Either \"system\", \"user\", or \"assistant\".\n",
    "  - **content**: The text for the message.\n",
    "- **max_tokens**: Controls how many tokens to generate.\n",
    "- **temperature**: Controls randomness in the output (higher value = more random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240af0f0-38e1-494f-96d9-06df7e845d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d0ecee-43ca-451c-948f-5f5f754a4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # api_key = openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10908d09-2e84-40c9-aa48-8d8f43f81ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to the Chat Completion Endpoint\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Specify the model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Can you summarize the key parameters for the Chat Completion Endpoint?\"}\n",
    "    ],\n",
    "    max_tokens=50,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7aeb00-fc23-4229-a2e8-e32a551a1633",
   "metadata": {},
   "source": [
    "In OpenAI's API for language models, messages are the fundamental building blocks of prompts. \n",
    "\n",
    "Each message typically consists of two main components: the `role` of the sender and the `content` of the message. \n",
    "\n",
    "| **Role**   | **Description**                                                                                                                                                 | **Example**                                                                                                                  |\n",
    "|------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n",
    "| User       | Represents the end user or the application initiating the conversation. Sends input messages, asking questions or providing information for the model's response. | If a user asks, \"What is machine learning?\", the message is sent with the role of \"user\".                                  |\n",
    "| Assistant  | Represents the AI model's responses to the user’s queries. Generates replies based on the context and content of previous messages, aiming to be helpful and informative. | After the user asks about machine learning, the assistant might respond, \"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data.\" |\n",
    "| System     | Provides initial instructions or context to the assistant to guide its behavior throughout the interaction. Typically used to set the tone, rules, or constraints of the assistant’s responses. | A system message might state, \"You are a friendly and informative assistant.\"                                              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc75f140-b420-4102-8368-d04efe93c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a knowledgeable assistant.'},\n",
       " {'role': 'user', 'content': 'Can you tell me about neural networks?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Neural networks are a series of algorithms that mimic                                     the operations of a human brain to recognize relationships in a set of data.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "  {\"role\": \"system\",    \"content\": \"You are a knowledgeable assistant.\"},\n",
    "  {\"role\": \"user\",      \"content\": \"Can you tell me about neural networks?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Neural networks are a series of algorithms that mimic \\\n",
    "                                    the operations of a human brain to recognize relationships in a set of data.\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0fbc3-1998-47b1-a870-7731c4edde72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
