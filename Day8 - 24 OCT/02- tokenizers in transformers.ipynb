{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87af0b4-e6c6-43fe-a54d-1a0294d5b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7848276-d14b-4e1f-a86a-4dcf58d6b96f",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "## BPE \n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0afcd3-54fe-476a-9d15-8efea2b944ff",
   "metadata": {},
   "source": [
    "#### GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497d1799-ac9b-4844-ba59-5e1db98da757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdadbf2-39c6-402c-878a-c604857ff033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206e7f08-08c5-4df3-b45c-f0bd36eed8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text you want to tokenize\n",
    "text = \"Hello, how are you? I am learning about GPT models.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3732b3f1-d21b-46e9-947a-459d0ff71ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?', 'ĠI', 'Ġam', 'Ġlearning', 'Ġabout', 'ĠG', 'PT', 'Ġmodels', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9521ced3-a3ab-45f1-82e0-9b9361737115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [15496, 11, 703, 389, 345, 30, 314, 716, 4673, 546, 402, 11571, 4981, 13]\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to their corresponding input IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "972c925c-89b7-4ba9-9ffb-75e08419a3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: Hello, how are you? I am learning about GPT models.\n"
     ]
    }
   ],
   "source": [
    "# Convert back to the original text (detokenization)\n",
    "decoded_text = tokenizer.decode(input_ids)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbd47c-fcd1-40ee-aa39-722545106389",
   "metadata": {},
   "source": [
    "**Explanation of why tokens look like this:**\n",
    "\n",
    "`['Hello', ',', 'Ġhow', 'Ġare', 'Ġyou', '?', 'ĠI', 'Ġam', 'Ġlearning', 'Ġabout', 'ĠG', 'PT', 'Ġmodels', '.']`\n",
    "\n",
    "\"\"\"\n",
    "1. 'Ġ' Represents a Space:\n",
    "   - The 'Ġ' symbol you see before tokens like 'Ġhow', 'Ġare', and 'Ġyou' represents a space before the word.\n",
    "   - This is because GPT-2 uses Byte Pair Encoding (BPE), which does not tokenize text at the word level, but at the subword level.\n",
    "   - For example, instead of tokenizing \" how\", \" are\", and \" you\" as whole words, the tokenizer marks a space with 'Ġ' to denote \n",
    "     that a space precedes those subword tokens.\n",
    "\n",
    "2. Subword Tokenization:\n",
    "   - The BPE tokenizer breaks words into smaller subword units based on their frequency in the training data.\n",
    "   - For example, \"GPT\" is tokenized into ['ĠG', 'PT'] because \"GPT\" is less frequent and split into more common subword components.\n",
    "\n",
    "3. Efficient Vocabulary Size:\n",
    "   - BPE helps keep the vocabulary size manageable by encoding words as a combination of subwords.\n",
    "   - This makes it easier to handle unseen words by decomposing them into subwords known by the model.\n",
    "\n",
    "4. Example Breakdown:\n",
    "   - 'Hello': The entire word appears as one token because it's frequent in the training data.\n",
    "   - 'Ġhow', 'Ġare', 'Ġyou': These tokens have the space marker 'Ġ' indicating they follow a space.\n",
    "   - 'ĠG', 'PT': The word \"GPT\" is split into subwords since it's relatively rare in the training data.\n",
    "   - 'Ġmodels': The token has 'Ġ' to indicate it follows a space.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c529a9d-11f6-44de-b301-6871f2e0a075",
   "metadata": {},
   "source": [
    "#### GPT-3 Tokenizer (Using tiktoken)\n",
    "For GPT-3 and GPT-4 models, the tiktoken library is used for tokenization, which is faster and more efficient than the previous methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e17c48-daa7-483e-89b8-98ef08755a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b34752-cc70-4398-a0c5-448648e0faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-3 tokenizer (uses tiktoken)\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497f17ee-1935-4f7a-a2ed-435a47da3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text\n",
    "tokens_gpt3 = encoding.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11c916d-caa3-4e96-b9b9-31afe0243668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 Tokens (IDs): [9906, 11, 1268, 527, 499, 30, 358, 1097, 6975, 922, 480, 2898, 4211, 13]\n",
      "GPT-3 Token Words: ['Hello', ',', ' how', ' are', ' you', '?', ' I', ' am', ' learning', ' about', ' G', 'PT', ' models', '.']\n"
     ]
    }
   ],
   "source": [
    "# Convert token IDs back to their corresponding token words\n",
    "token_words_gpt3 = [encoding.decode([token]) for token in tokens_gpt3]\n",
    "\n",
    "print(\"GPT-3 Tokens (IDs):\", tokens_gpt3)\n",
    "print(\"GPT-3 Token Words:\", token_words_gpt3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6615d416-f002-4819-bad9-00b5fd649b81",
   "metadata": {},
   "source": [
    "**GPT-3 often produces fewer tokens overall due to more efficient tokenization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b4d2c-d22d-4033-a27c-b242252b874b",
   "metadata": {},
   "source": [
    "#### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fa755a-af71-475e-a031-a4148527b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Load the pre-trained RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path = \"roberta-base\",\n",
    "                                             cache_dir = r'D:\\AI-DATASETS\\07-Hugging-Face-Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d768daf-227c-4cec-8f4d-02edd52fb116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [0, 31414, 6, 141, 32, 47, 116, 38, 524, 2239, 59, 272, 10311, 3092, 4, 2]\n",
      "Token Words: ['<s>', 'Hello', ',', ' how', ' are', ' you', '?', ' I', ' am', ' learning', ' about', ' G', 'PT', ' models', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input text (returns token IDs)\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "# Convert token IDs back to corresponding token words\n",
    "token_words = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "# Print the token IDs and corresponding token words\n",
    "print(\"Token IDs:\", tokens)\n",
    "print(\"Token Words:\", token_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1c0ee-f1cf-48f3-b578-2034e837d8a6",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "## Wordpiece\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d887d7-2ff7-4ab0-8001-ba637575dca1",
   "metadata": {},
   "source": [
    "#### Python Code for BERT Tokenizer (WordPiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e06e0ed-4e8e-4eaf-9764-e6e30714d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2935fa9f-95dc-464f-86b2-6d584718c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT tokenizer (uses WordPiece)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = \"bert-base-uncased\",\n",
    "                                          cache_dir = r'D:\\AI-DATASETS\\07-Hugging-Face-Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb7f07ba-6275-401a-80ca-8d917d48d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input text (returns token IDs)\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b12fb7a8-c467-4cb5-a6c5-e65980a98e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [101, 7592, 1010, 2129, 2024, 2017, 1029, 1045, 2572, 4083, 2055, 14246, 2102, 4275, 1012, 102]\n",
      "Token Words: ['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', 'i', 'am', 'learning', 'about', 'gp', '##t', 'models', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Convert token IDs back to corresponding token words\n",
    "token_words = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "# Print the token IDs and corresponding token words\n",
    "print(\"Token IDs:\", tokens)\n",
    "print(\"Token Words:\", token_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6e00f-236b-4ad7-b609-83cbb73efd53",
   "metadata": {},
   "source": [
    "`encode(text, add_special_tokens=True)`: \n",
    "- Encodes the input text into token IDs and adds special tokens like [CLS] and [SEP] at the beginning and end of the sequence, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a73e4-91e9-4004-a400-9f8de9550ca3",
   "metadata": {},
   "source": [
    "#### DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb95106d-b283-46db-8cec-11ea657f2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Load the pre-trained DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                cache_dir = r'D:\\AI-DATASETS\\07-Hugging-Face-Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16fb6157-970f-47a2-8f18-55880fe2a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [101, 7592, 1010, 2129, 2024, 2017, 1029, 1045, 2572, 4083, 2055, 14246, 2102, 4275, 1012, 102]\n",
      "Token Words: ['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', 'i', 'am', 'learning', 'about', 'gp', '##t', 'models', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input text (returns token IDs)\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "# Convert token IDs back to corresponding token words\n",
    "token_words = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "# Print the token IDs and corresponding token words\n",
    "print(\"Token IDs:\", tokens)\n",
    "print(\"Token Words:\", token_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319ef31-f71a-484f-aef2-40a0ef17fbeb",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## SentencePiece \n",
    "---------------------------\n",
    "- is a tokenization algorithm used by models like T5, ALBERT, and XLM-R.\n",
    "- It differs from BPE and WordPiece as it treats text as a sequence of characters without needing pre-tokenization (such as splitting by spaces).\n",
    "- SentencePiece can operate using subword units, providing a robust way to handle both rare and common tokens in any language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67adab-429c-49a5-87cc-10c35932f471",
   "metadata": {},
   "source": [
    "#### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18ac490d-c2bc-41d4-99ff-58db3209213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load the pre-trained T5 tokenizer (uses SentencePiece)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\",\n",
    "                                        cache_dir = r'D:\\AI-DATASETS\\07-Hugging-Face-Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6abcccfe-a6c8-4880-9d53-7f64a2c86e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [8774, 6, 149, 33, 25, 58, 27, 183, 1036, 81, 350, 6383, 2250, 5, 1]\n",
      "Token Words: ['Hello', ',', 'how', 'are', 'you', '?', 'I', 'am', 'learning', 'about', 'G', 'PT', 'models', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input text (returns token IDs)\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "# Convert token IDs back to corresponding token words\n",
    "token_words = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "# Print the token IDs and corresponding token words\n",
    "print(\"Token IDs:\", tokens)\n",
    "print(\"Token Words:\", token_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16753657-64b4-4257-a0a6-8326aba0ad53",
   "metadata": {},
   "source": [
    "#### Albert - with [CLS] and [SEP] Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e1c8448-c8bc-4214-9f1f-5ee0e3e2ed27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e14a82c6004663a6b30d465a0ef5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\AI-DATASETS\\07-Hugging-Face-Data\\models--albert-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb35e7d1867f47528a9d479548f6a1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb845c59146540d8a5fc696b405be2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f02cc4a265e4efcbc466a6fef55a325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "\n",
    "# Load the pre-trained ALBERT tokenizer (SentencePiece-based)\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\",\n",
    "                                           cache_dir = r'D:\\AI-DATASETS\\07-Hugging-Face-Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c913b95-80e6-4fb0-8a4d-fb809a4032d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [2, 10975, 15, 184, 50, 42, 60, 31, 589, 2477, 88, 10538, 38, 2761, 9, 3]\n",
      "Token Words: ['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', 'i', 'am', 'learning', 'about', 'gp', 't', 'models', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input text (returns token IDs)\n",
    "tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "# Convert token IDs back to corresponding token words\n",
    "token_words = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "# Print the token IDs and corresponding token words\n",
    "print(\"Token IDs:\", tokens)\n",
    "print(\"Token Words:\", token_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425ce8d-cef9-41e7-b7ff-356d10805fd5",
   "metadata": {},
   "source": [
    "Another example ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "764502c1-6b9a-4ed9-b512-b68fc8f4e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words categorized by complexity\n",
    "words = [\n",
    "    \"cat\",                      # Simple\n",
    "    \"happy\",                    # Simple\n",
    "    \"unhappiness\",              # Medium complexity\n",
    "    \"international\",            # Medium complexity\n",
    "    \"computerization\",          # Medium complexity\n",
    "    \"antidisestablishmentarianism\",  # Complex\n",
    "    \"electroencephalographically\",    # Complex\n",
    "    \"pseudopseudohypoparathyroidism\"  # Complex\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da59acc9-7859-4f79-b065-27aba6d2b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: cat\n",
      "Tokens: ['▁cat']\n",
      "----------------------------------------\n",
      "Word: happy\n",
      "Tokens: ['▁happy']\n",
      "----------------------------------------\n",
      "Word: unhappiness\n",
      "Tokens: ['▁un', 'hap', 'pi', 'ness']\n",
      "----------------------------------------\n",
      "Word: international\n",
      "Tokens: ['▁international']\n",
      "----------------------------------------\n",
      "Word: computerization\n",
      "Tokens: ['▁computer', 'ization']\n",
      "----------------------------------------\n",
      "Word: antidisestablishmentarianism\n",
      "Tokens: ['▁anti', 'dis', 'establishment', 'arian', 'ism']\n",
      "----------------------------------------\n",
      "Word: electroencephalographically\n",
      "Tokens: ['▁electro', 'en', 'cephal', 'ographic', 'ally']\n",
      "----------------------------------------\n",
      "Word: pseudopseudohypoparathyroidism\n",
      "Tokens: ['▁pseudo', 'ps', 'e', 'udo', 'hy', 'po', 'para', 'thy', 'roid', 'ism']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print the results\n",
    "for word in words:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a18746-63a1-4a7d-8fde-462235c98bd6",
   "metadata": {},
   "source": [
    "#### How Tokenization Works\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - **Data Collection**: The tokenizer is trained on a large corpus of text data. This corpus can include books, articles, and any relevant textual data.\n",
    "   - **Frequency Analysis**: During training, the tokenizer analyzes the frequency of all possible character sequences (up to a certain length). It identifies commonly occurring subword units, words, or characters.\n",
    "   - **Subword Units Creation**:\n",
    "     - **WordPiece**: It starts with individual characters and gradually combines them to form larger units based on their frequency. The most common sequences are kept as tokens, and rare sequences are split into smaller parts. This process continues until a predefined vocabulary size is reached.\n",
    "     - **SentencePiece**: It operates more like a language model. It uses a statistical approach to segment text into subword units, without relying on whitespace as a boundary. It treats the entire corpus as a sequence and learns to find the most efficient segmentation based on a byte pair encoding (BPE) or unigram language model.\n",
    "\n",
    "2. **Vocabulary Creation**: After training, the tokenizer has a fixed vocabulary that contains:\n",
    "   - Whole words (e.g., \"cat\", \"happy\")\n",
    "   - Subwords (e.g., \"un\", \"happiness\", \"anti\", \"dis\", \"est\")\n",
    "   - Special tokens (e.g., `[CLS]`, `[SEP]`)\n",
    "\n",
    "3. **Tokenization During Inference**:\n",
    "   - When you pass a simple text string to the tokenizer (e.g., `\"Antidisestablishmentarianism\"`), it uses the vocabulary created during the training phase to segment the input text into tokens.\n",
    "   - The tokenizer matches substrings of the input text against its vocabulary:\n",
    "     - It starts with the longest substring that matches a token in its vocabulary.\n",
    "     - If the entire word isn’t found, it attempts to break it down into smaller parts, looking for known subwords.\n",
    "   - This process continues until the entire string is tokenized into valid tokens that correspond to IDs in the vocabulary.\n",
    "\n",
    "#### Example Process\n",
    "Let’s illustrate this process step-by-step using the example of the word **\"unhappiness\"**.\n",
    "\n",
    "1. **WordPiece Example**:\n",
    "   - **Input**: `\"unhappiness\"`\n",
    "   - **Tokenization Steps**:\n",
    "     - Check if the entire word is in the vocabulary: **Not found**.\n",
    "     - Check for subwords:\n",
    "       - Match **\"un\"** (found in vocabulary).\n",
    "       - Match **\"happiness\"** (found in vocabulary).\n",
    "   - **Output Tokens**: `[\"un\", \"happiness\"]`\n",
    "\n",
    "2. **SentencePiece Example**:\n",
    "   - **Input**: `\"unhappiness\"`\n",
    "   - **Tokenization Steps**:\n",
    "     - SentencePiece does not assume any delimiters. It analyzes the word as a whole.\n",
    "     - Using its learned segmentation, it might identify:\n",
    "       - Tokenized as `[\"▁un\", \"happiness\"]` (the \"▁\" indicates a word boundary).\n",
    "   - **Output Tokens**: `[\"▁un\", \"happiness\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e2625-3445-4214-bb45-f28e9f2aef56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
