{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca64a5e6-0e33-42f1-b761-6166451af210",
   "metadata": {},
   "source": [
    "#### Contrastive Language-Image Pretraining (CLIP)\n",
    "- provides a similarity score that indicates how well an image matches a given text prompt.\n",
    "- For example, the score `tensor([0.3093])` represents the similarity between the image and text in a shared embedding space, with `higher` values indicating `stronger alignment`.\n",
    "\n",
    "##### How CLIP Works\n",
    "CLIP (Contrastive Language-Image Pretraining) is a `multimodal` model designed to measure the similarity between `images` and `text prompts`. Here’s how it works:\n",
    "\n",
    "- **Image Encoder**: Transforms images into vector embeddings using models like ResNet or Vision Transformers (ViT).\n",
    "- **Text Encoder**: Transforms text prompts into vector embeddings using a Transformer-based architecture.\n",
    "\n",
    "CLIP learns to align image and text embeddings in a shared space by `maximizing` the similarity of correct pairs (image-caption) and minimizing unrelated pairs. \n",
    "\n",
    "This training method enables CLIP to understand and align images with text descriptions effectively.\n",
    "\n",
    "##### How CLIP Predicts (Scores)\n",
    "To calculate a similarity score:\n",
    "1. **Encoding**: CLIP encodes both the image and text prompt into vector embeddings.\n",
    "2. **Cosine Similarity**: It calculates the cosine similarity between these vectors. The score ranges from -1 to 1:\n",
    "   - **Closer to 1**: Stronger alignment.\n",
    "   - **Closer to -1**: Weaker alignment.\n",
    "\n",
    "For example, a score of `0.3093` indicates moderate alignment between the image and prompt, suggesting room for improvement if higher alignment is desirable.\n",
    "\n",
    "##### Fine-Tuning CLIP\n",
    "Fine-tuning can help improve performance on specific datasets or domains but requires careful techniques to avoid overfitting:\n",
    "\n",
    "1. **Custom Datasets**: Fine-tune with a domain-specific dataset (e.g., medical images) to improve alignment in specific fields.\n",
    "2. **Contrastive Loss Adjustments**: Modifying contrastive loss can help re-optimize similarity sensitivity.\n",
    "3. **Learning Rate Adjustments**: Lower learning rates prevent the model from drifting too far from general representations.\n",
    "4. **Parameter Freezing**: Freeze layers in the encoder to preserve pretrained embeddings, modifying only specific layers.\n",
    "\n",
    "##### Best Practices for Using CLIP\n",
    "\n",
    "1. **Real-World Evaluation**: Validate CLIP’s similarity scores with real data to establish meaningful alignment thresholds.\n",
    "2. **Handle Low Scores Carefully**: Low similarity scores may require fine-tuning or testing different prompts/images.\n",
    "3. **High-Quality Prompts**: Clear and descriptive text prompts can improve CLIP’s alignment performance.\n",
    "4. **Bias Awareness**: CLIP’s internet-sourced dataset can introduce biases. Monitor applications for unintended effects.\n",
    "5. **Ensemble Approach**: Use CLIP alongside other models, like object detection or scene classification, for improved contextual accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2364fb-7878-4c9c-ab23-3032b28ab949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfadf3fa-a6a8-4e07-967c-caa575fa1920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43eabd03bb3446b4bb6892c181b848b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bhupe\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch16. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90c7a269ef34d11935d96429880e251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6bdee9d8834862b71967ed5a57b5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c923b734ac451a88c74b28d950f6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068c222768d74f03bb513d56244bf119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b32123622674a3f9e2e9a797e350048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2823c65756e34ec49becbb6ab7d8da34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb9154f99334e179af29952f041d632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model and processor\n",
    "model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37272713-e2d6-40ad-a2ae-947ee08d4085",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_used = '''\n",
    "Capture the essence of adventure with a stunning visual of a futuristic \n",
    "astronaut exploring a space station that overlooks Earth, bathed in warm, \n",
    "golden light; a simple and clean scene that evokes feelings of epic power, \n",
    "set against a backdrop of soft colors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f208de5-a33d-42c7-ace4-f412e565aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ad07e7-2a01-474d-b425-0daa8a84102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r'D:\\DOWNLOADS\\generated_image.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff43cf8-5861-47da-b8fd-f165804af142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image using PIL\n",
    "generated_image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ede3b124-cb71-4d93-92bb-8ffa8ff4c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the text and image\n",
    "inputs = processor(text          = prompt_used, \n",
    "                   images        = generated_image, \n",
    "                   return_tensors= \"pt\", \n",
    "                   padding       = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d836e51a-f554-4262-93c1-a254829d6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fa2e974-b714-49c8-a323-7009f0caf3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding  = outputs.text_embeds\n",
    "image_embedding = outputs.image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "716caca9-b01b-43d9-996a-78604515f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "similarity_score = cosine_similarity(text_embedding, image_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd7b052-900e-45f6-a5ff-8002ca1d4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3093], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352f913-2185-4b78-939e-23120a60052a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d7818-33a6-4adb-b5c9-36b7425cf962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf8636-b909-4df3-b68b-4a474e12fa8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
