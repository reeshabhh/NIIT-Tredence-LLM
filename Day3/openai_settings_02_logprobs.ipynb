{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3138b6f7-ff84-49b9-8dbb-e5147df5708b",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### Parameters of OpenAI LLM - log of prob\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d361bf-1401-4187-bd1e-25efe7433839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0611db-6cff-4df1-a3a7-53d1e2ed663a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.45.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c2985-e888-4153-85ba-3d29842ab9d6",
   "metadata": {},
   "source": [
    "#### Logprobs Attribute Summary\n",
    "\n",
    "| Logprobs | Description                                                                                              | Example                           |\n",
    "|----------|----------------------------------------------------------------------------------------------------------|-----------------------------------|\n",
    "| None     | Indicates that there are no associated log probabilities provided for the completion.                   | \"logprobs\": None                  |\n",
    "| {}       | Represents an empty dictionary, meaning no log probabilities were calculated for this response.         | \"logprobs\": {}                    |\n",
    "| {...}    | Contains a dictionary of log probabilities for each token in the generated text, if applicable.         | \"logprobs\": {\"tokens\": [...], \"token_logprobs\": [...], \"top_logprobs\": [...], \"text_offset\": [...] } |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a38c83-baab-4ebf-b5f8-b907c746be2d",
   "metadata": {},
   "source": [
    "- Use Cases for Logprobs Attribute with Example Prompts\n",
    "\n",
    "| Use Case                            | Prompt                                      | Output                                | Logprobs                                                                                                                                                                       | Interpretation                                                                                              |\n",
    "|-------------------------------------|---------------------------------------------|---------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n",
    "| Analyzing Model Confidence           | \"What is the capital of France?\"           | \"Paris\"                               | {\"tokens\": [\"Paris\"], \"token_logprobs\": [-0.1]}                                                                                                                             | A log probability of -0.1 indicates high confidence in \"Paris\" as the correct answer.                      |\n",
    "| Understanding Alternatives           | \"The sky is usually...\"                    | \"blue\"                                | {\"top_logprobs\": [{\"blue\": -0.5}, {\"gray\": -1.2}, {\"green\": -1.5}]}                                                                                                         | The model strongly preferred \"blue,\" with \"gray\" and \"green\" being less likely alternatives.                |\n",
    "| Improving Sampling Strategies        | \"A great place to relax is...\"             | \"the beach\"                           | {\"tokens\": [\"the\", \"beach\"], \"token_logprobs\": [-0.3, -1.0], \"top_logprobs\": [{\"the beach\": -0.3}, {\"a park\": -1.5}, {\"a forest\": -1.8}]}                                 | The model's choice of \"the beach\" indicates a relatively high confidence, suggesting effective sampling.     |\n",
    "| Evaluating Output Quality            | \"What is 2 + 2?\"                           | \"4\"                                   | {\"tokens\": [\"4\"], \"token_logprobs\": [-0.2]}                                                                                                                                 | A low log probability for \"4\" suggests that the model is likely providing a correct and reliable answer.      |\n",
    "| Training and Fine-tuning Insights   | \"The cat chased the...\"                    | \"mouse\"                               | {\"tokens\": [\"mouse\"], \"token_logprobs\": [-2.0]}                                                                                                                             | A low log probability indicates the model struggled to associate \"cat\" with \"mouse,\" signaling a training gap.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2933bd-3b92-4ab7-8bbb-f8f7720ae360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'text': 'Hello, world!',\n",
       "   'logprobs': {'tokens': ['Hello', ',', 'world', '!'],\n",
       "    'token_logprobs': [-0.1, -0.2, -0.3, -0.4],\n",
       "    'top_logprobs': [Ellipsis]}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"text\": \"Hello, world!\",\n",
    "      \"logprobs\": {\n",
    "        \"tokens\"        : [\"Hello\", \",\", \"world\", \"!\"],\n",
    "        \"token_logprobs\": [-0.1, -0.2, -0.3, -0.4],\n",
    "        \"top_logprobs\"  : [...]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c15efbe-ff4a-440e-80f6-80e7ac3a87f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'text': 'Hello, world!',\n",
       "   'logprobs': {'tokens': ['Hello', ',', 'world', '!'],\n",
       "    'token_logprobs': [-0.1, -0.2, -0.3, -0.4],\n",
       "    'top_logprobs': [{'Hello': -0.1, 'Hi': -0.15, 'Hey': -0.2},\n",
       "     {',': -0.2, '.': -0.3, '-': -0.35},\n",
       "     {'world': -0.3, 'earth': -0.35, 'globe': -0.4},\n",
       "     {'!': -0.4, '.': -0.5, '?': -0.55}]}}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"text\": \"Hello, world!\",\n",
    "      \"logprobs\": {\n",
    "        \"tokens\"        : [\"Hello\", \",\", \"world\", \"!\"],\n",
    "        \"token_logprobs\": [-0.1, -0.2, -0.3, -0.4],\n",
    "        \"top_logprobs\": [\n",
    "          {\"Hello\": -0.1, \"Hi\": -0.15, \"Hey\": -0.2},\n",
    "          {\",\": -0.2, \".\": -0.3, \"-\": -0.35},\n",
    "          {\"world\": -0.3, \"earth\": -0.35, \"globe\": -0.4},\n",
    "          {\"!\": -0.4, \".\": -0.5, \"?\": -0.55}\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a573773c-105f-4428-8a61-42afad8f9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589a9572-6b21-46fa-8402-08fb8fc1f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key = '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'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1c0faf-93df-4040-817c-c5478807694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with master level in General KNowledge.\"},\n",
    "        {\"role\": \"user\",   \"content\": '''What is the capital of Punjab state in India? Only return the name'''},\n",
    "    ],\n",
    "    logprobs = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c6a528e-425f-4ead-89e4-6e0de6262ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'stop',\n",
       " 'index': 0,\n",
       " 'logprobs': {'content': [{'token': 'Ch',\n",
       "    'bytes': [67, 104],\n",
       "    'logprob': -1.735894e-05,\n",
       "    'top_logprobs': []},\n",
       "   {'token': 'and',\n",
       "    'bytes': [97, 110, 100],\n",
       "    'logprob': -4.3202e-07,\n",
       "    'top_logprobs': []},\n",
       "   {'token': 'igarh',\n",
       "    'bytes': [105, 103, 97, 114, 104],\n",
       "    'logprob': -4.9617593e-06,\n",
       "    'top_logprobs': []}],\n",
       "  'refusal': None},\n",
       " 'message': {'content': 'Chandigarh', 'refusal': None, 'role': 'assistant'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view logprobs\n",
    "response.to_dict()['choices'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5b15073-3157-4b43-a593-adb41c3279a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': [{'token': 'Ch',\n",
       "   'bytes': [67, 104],\n",
       "   'logprob': -1.735894e-05,\n",
       "   'top_logprobs': []},\n",
       "  {'token': 'and',\n",
       "   'bytes': [97, 110, 100],\n",
       "   'logprob': -4.3202e-07,\n",
       "   'top_logprobs': []},\n",
       "  {'token': 'igarh',\n",
       "   'bytes': [105, 103, 97, 114, 104],\n",
       "   'logprob': -4.9617593e-06,\n",
       "   'top_logprobs': []}],\n",
       " 'refusal': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.to_dict()['choices'][0]['logprobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0352fb9-60da-4ece-a854-d91c898516b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import math\n",
    "\n",
    "# # Manually create log-probabilities (from -10 to 0)\n",
    "# log_probs = [-10 + i * (10 / 99) for i in range(100)]  # 100 points between -10 and 0\n",
    "\n",
    "# # Calculate the corresponding probabilities (confidence)\n",
    "# probs = [math.exp(lp) for lp in log_probs]\n",
    "\n",
    "# # Plotting the relationship between log-probabilities and probabilities\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.plot(log_probs, probs, label='Confidence (Probability)', color='b')\n",
    "\n",
    "# # Adding labels and title\n",
    "# plt.title('Correlation Between Log-prob and Probability (Confidence)')\n",
    "# plt.xlabel('Log-probability')\n",
    "# plt.ylabel('Probability (Confidence)')\n",
    "\n",
    "# # Show grid\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f57a1-4ff1-4872-83cd-e48315ffc396",
   "metadata": {},
   "source": [
    "#### Why use log of probs\n",
    "\n",
    "- `logprobs` stands for logarithm of probabilities. It provides the log-probability (logarithmic scale) of each token generated by the model.\n",
    "- These values represent how `likely` the model thinks a particular token should come next, with lower values indicating more probable tokens.\n",
    "- `Log-probabilities` are often preferred because they are more `numerically stable` than raw probabilities, especially when dealing with `very small probability values`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b09fca-c51f-4566-9e64-7cdabecac8f1",
   "metadata": {},
   "source": [
    "**Final logprob or probability for the final answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26e9abf3-dc58-4d6f-a16b-84f3a63933f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "109550ee-b67d-46ef-967f-b31a18dacda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_logprob_and_prob(data):\n",
    "    # Extract log-probabilities from the input data\n",
    "    logprobs_content = data['logprobs']['content']\n",
    "    \n",
    "    # Initialize cumulative log-probability\n",
    "    cumulative_logprob = 0\n",
    "    \n",
    "    # Iterate over each token and sum the log-probabilities\n",
    "    for token_info in logprobs_content:\n",
    "        cumulative_logprob += token_info['logprob']\n",
    "    \n",
    "    # Calculate the final probability (confidence)\n",
    "    final_prob = np.exp(cumulative_logprob)\n",
    "    \n",
    "    return cumulative_logprob, final_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47be9e2d-4eb5-4030-9b9b-33be2aae5589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cumulative log-probability for 'Kolkata' is: -2.27527193e-05\n",
      "The probability (confidence) for 'Kolkata' is: 0.9999772475395412\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the final log-probability and probability\n",
    "final_logprob, final_prob = calculate_final_logprob_and_prob(response.to_dict()['choices'][0])\n",
    "print(f\"The cumulative log-probability for 'Kolkata' is: {final_logprob}\")\n",
    "print(f\"The probability (confidence) for 'Kolkata' is: {final_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312adc40-f0ad-41ce-beab-d8af479a4c22",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1da84c42-7397-4d61-b6d5-7d6ba397855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text retrieved\n",
    "text_retrieved = \"\"\"Augusta Ada King, Countess of Lovelace (née Byron; 10 December 1815 – 27 November 1852) was an \n",
    "English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose \n",
    "computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure \n",
    "calculation.\n",
    "Ada Byron was the only legitimate child of poet Lord Byron and reformer Lady Byron. All Lovelace's half-siblings, \n",
    "Lord Byron's other children, were born out of wedlock to other women. Byron separated from his wife a month after \n",
    "Ada was born and left England forever. He died in Greece when Ada was eight. Her mother was anxious about her \n",
    "upbringing and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing her \n",
    "father's perceived insanity. Despite this, Ada remained interested in him, naming her two sons Byron and Gordon. \n",
    "\n",
    "Upon her death, she was buried next to him at her request. Although often ill in her childhood, Ada pursued her studies \n",
    "assiduously. She married William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess \n",
    "of Lovelace.\n",
    "Her educational and social exploits brought her into contact with scientists such as Andrew Crosse, Charles Babbage,\n",
    "Sir David Brewster, Charles Wheatstone, Michael Faraday, and the author Charles Dickens, contacts which she used to \n",
    "further her education. Ada described her approach as \"poetical science\" and herself as an \"Analyst (& Metaphysician)\".\n",
    "When she was eighteen, her mathematical talents led her to a long working relationship and friendship with fellow \n",
    "British mathematician Charles Babbage, who is known as \"the father of computers\". She was in particular interested \n",
    "in Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and \n",
    "her private tutor, Mary Somerville.\n",
    "Between 1842 and 1843, Ada translated an article by the military engineer Luigi Menabrea (later Prime Minister of Italy) \n",
    "about the Analytical Engine, supplementing it with an elaborate set of seven notes, simply called \"Notes\".\n",
    "Lovelace's notes are important in the early history of computers, especially since the seventh one contained \n",
    "what many consider to be the first computer program—that is, an algorithm designed to be carried out by a machine. \n",
    "Other historians reject this perspective and point out that Babbage's personal notes from the years 1836/1837 \n",
    "contain the first programs for the engine. She also developed a vision of the capability of computers to go \n",
    "beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. Her mindset of \"poetical science\" led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7edae7d7-c5d7-4d03-a82c-6909c9398aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_questions = [\n",
    "    \"Was Ada Lovelace known for her work on Charles Babbage's Analytical Engine?\",\n",
    "    \"Was Ada Lovelace buried next to her father, Lord Byron, at her request ?\",\n",
    "]\n",
    "\n",
    "diff_questions = [\n",
    "    \"   \",\n",
    "    \"   \",  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ec6be55-90ff-422b-9879-b62b7c534f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You retrieved this article: {article}. The question is: {question}.\n",
    "Before even answering the question, consider whether you have sufficient information in the article to answer the question fully.\n",
    "Your output should JUST be the boolean true or false, of if you have sufficient information in the article to answer the question.\n",
    "Respond with just one word, the boolean true or false. You must output the word 'True', or the word 'False', nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27f17b8d-50a5-4fbb-a649-fc46171a5fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qs :  Was Ada Lovelace known for her work on Charles Babbage's Analytical Engine?\n",
      "\tToken : True, logprobs : -4.3202e-07,  probability: 100.0\n",
      "Qs :  Was Ada Lovelace buried next to her father, Lord Byron, at her request ?\n",
      "\tToken : True, logprobs : -4.3202e-07,  probability: 100.0\n"
     ]
    }
   ],
   "source": [
    "# loop thru the qs\n",
    "# for each qs, fire the query (chatcompletion) to the OpenAI LLM\n",
    "# extract the log prob\n",
    "for question in easy_questions:\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt.format(article=text_retrieved, question=question)}\n",
    "        ],\n",
    "        logprobs = True,\n",
    "    )\n",
    "\n",
    "    print('Qs : ', question)\n",
    "    \n",
    "    for logprob in response.choices[0].logprobs.content:\n",
    "\n",
    "        print(f'\\tToken : {logprob.token}, logprobs : {logprob.logprob},  probability: {np.round(np.exp(logprob.logprob)*100,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd20b0-14b2-4aba-8466-84c00fe23fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
