{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e933bc-eb41-4f1b-9434-a8fde4cdc61f",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### Settings - logit_bias\n",
    "\n",
    "- Optional , Defaults to null\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecbc2c7-9141-4edf-85e3-34cf81249368",
   "metadata": {},
   "source": [
    "Logit bias in OpenAI's GPT models is a powerful tool for influencing the likelihood of specific tokens in the generated output. \n",
    "\n",
    "It allows fine-grained control over the model's behavior by adjusting the probability of specific token outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b058ef-e0c3-419c-8bd1-fc47696d0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7a6a98-1c07-4d93-b933-4b78b83d3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756099c6-542b-4ad0-8351-1e63b8cc0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # api_key = openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9192813-15a1-4766-9e59-a4cfb61d6bc6",
   "metadata": {},
   "source": [
    "#### Example - 01\n",
    "\n",
    "**Controlling Specific Output Formats**\n",
    "- Yes/No Responses: In tasks where only simple answers like \"Yes\" or \"No\" are acceptable (e.g., binary questions), logit_bias can ensure the model prefers these responses.\n",
    "- Example: \"Should I invest in this stock?\" â€“ Boosting the likelihood of \"Yes\" and \"No\" to ensure clear binary responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea9d757-ee8e-42c5-a20c-862f60721c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs for 'Yes': [9642]\n",
      "Token IDs for 'No': [2822]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize tokenizer\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Get token IDs for \"Yes\" and \"No\"\n",
    "yes_tokens = enc.encode(\"Yes\")\n",
    "no_tokens  = enc.encode(\"No\")\n",
    "\n",
    "print(f\"Token IDs for 'Yes': {yes_tokens}\")\n",
    "print(f\"Token IDs for 'No': {no_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc1bf45-c861-4169-ba85-7bf1b7e399a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text \n",
    "def generate_story(prompt, logit_bias, apply_logit_bias = 'N' ):\n",
    "\n",
    "    if apply_logit_bias.upper() == 'Y':\n",
    "        response = client.chat.completions.create(\n",
    "            model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "            messages   = [\n",
    "                {\"role\": \"assistant\", \"content\": 'You are a helpful assistant'},\n",
    "                {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "            ],\n",
    "            max_tokens = 3, \n",
    "            logit_bias = logit_bias,   # dict format\n",
    "            stop       = [\"\\n\"]\n",
    "        )\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "            messages   = [\n",
    "                {\"role\": \"assistant\", \"content\": 'You are a helpful assistant'},\n",
    "                {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "            ],\n",
    "            max_tokens = 3, \n",
    "            #logit_bias = logit_bias,\n",
    "            stop       = [\"\\n\"]\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2161183f-230d-4296-bac1-c7c777b5f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit bias to favor \"no\" and discourage \"yes\"\n",
    "# -100 to + 100\n",
    "# Good point to start is -10 or +10\n",
    "logit_bias = {\n",
    "    9642: -15,   # Token ID for \"yes\"\n",
    "    2822:  10    # Token ID for \"no\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4866a05d-ef9f-4fa0-bd42-487024730a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt to simulate a chatbot response\n",
    "prompt = \"Should I invest in this stock? Answer with only Yes or No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8741f3ac-f050-4c4d-86f4-fc1ce2749f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = generate_story(prompt, logit_bias, 'Y')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c16cba-b986-4412-bfc6-d290da502dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the story generation in a loop\n",
    "def run_iterations(prompt, logit_bias, n_iters):\n",
    "    results = []  # List to store the generated texts\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        text = generate_story(prompt, logit_bias, apply_logit_bias='Y')\n",
    "        results.append(text)  # Append each generated text to the list\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a9b95da-32ba-4650-bb22-14fab9d65759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8827481-8aca-468c-ada8-f979bc50524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text occurrences:\n",
      " Counter({'No': 10})\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations\n",
    "n_iters = 10\n",
    "\n",
    "# Run the generation in a loop\n",
    "generated_texts = run_iterations(prompt, logit_bias, n_iters)\n",
    "\n",
    "# Count the occurrences of unique values\n",
    "unique_counts = Counter(generated_texts)\n",
    "\n",
    "# Display the results\n",
    "print(\"Generated text occurrences:\\n\", unique_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20875a2-c82d-4c02-b653-aa3eda064bd8",
   "metadata": {},
   "source": [
    "#### Example 02\n",
    "Restricting Inappropriate or Unwanted Words\n",
    "\n",
    "Content Moderation: Logit_bias can prevent the model from generating specific words, phrases, or content that might be sensitive, inappropriate, or undesirable.\n",
    "\n",
    "Example: Suppressing offensive words or any word you don't want to appear by giving their token a very negative bias (e.g., -100).\n",
    "Avoiding Repetitive Phrases: If the model tends to repeat certain phrases, you can apply a negative bias to reduce their likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeadb219-3c33-476f-919b-b04b1ad535cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the words you want to suppress\n",
    "restricted_words = [\"bad\", \"terrible\"]\n",
    "logit_bias       = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f421fe30-64a2-44a8-b8eb-4444fabb5660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14176: -10, 466: -10, 12560: -10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the token IDs for the restricted words and apply a negative bias (-100)\n",
    "for word in restricted_words:\n",
    "    token_ids = enc.encode(word)\n",
    "    for token_id in token_ids:\n",
    "        logit_bias[token_id] = -10  # Heavily discourage these tokens\n",
    "\n",
    "logit_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8247486b-62a2-4fbb-9bf7-7e983f76cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text \n",
    "def generate_story(prompt, logit_bias, apply_logit_bias = 'N', max_tokens=100 ):\n",
    "\n",
    "    if apply_logit_bias.upper() == 'Y':\n",
    "        response = client.chat.completions.create(\n",
    "            model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "            messages   = [\n",
    "                {\"role\": \"assistant\", \"content\": 'You are a helpful assistant'},\n",
    "                {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "            ],\n",
    "            max_tokens = max_tokens, \n",
    "            logit_bias = logit_bias,\n",
    "            #stop       = [\"\\n\"]\n",
    "        )\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "            messages   = [\n",
    "                {\"role\": \"assistant\", \"content\": 'You are a helpful assistant'},\n",
    "                {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "            ],\n",
    "            max_tokens = max_tokens, \n",
    "            #logit_bias = logit_bias,\n",
    "            #stop       = [\"\\n\"]\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f5e4eec-ab73-453a-aa48-844a2ff62615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt where the model might use \"bad\" or \"terrible\"\n",
    "prompt = \"Given the poor reviews I've heard, provide an example of poor and full of anger product review?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "262cb75c-3afe-49c5-afbf-d9df0e7741c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: Sure, here's an example of a harsh and angry product review:\n",
      "\n",
      "\"I am utterly disgusted with this product. It's an absolute waste of money! The quality is awful, the design is ugly, and it doesn't even work properly. I can't believe I fell for the marketing hype and bought this piece of junk. Save your money and steer clear of this terrible product. I wouldn't recommend it to my worst enemy!\"\n"
     ]
    }
   ],
   "source": [
    "# Generate the response\n",
    "response_text = generate_story(prompt, logit_bias, apply_logit_bias='N')\n",
    "print(\"Generated Response:\", response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9cabfa2-4725-4510-b99c-14eae4c457e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: Sure, here is an example of a poor and angry product review:\n",
      "\n",
      "\"This product is an absolute disgrace! It is cheaply made, falls apart after just a few uses, and is a complete waste of money. I can't believe how terrible this product is, I will never buy from this company again. Save your money and avoid this piece of junk at all costs! I want a refund for this garbage. Absolutely infuriating!!\"\n"
     ]
    }
   ],
   "source": [
    "# Generate the response\n",
    "response_text = generate_story(prompt, logit_bias, apply_logit_bias='Y')\n",
    "print(\"Generated Response:\", response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f716a95-df61-4be3-82ef-59b4642312b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the words you want to suppress\n",
    "restricted_words = [\"scam\", \"junk\"]\n",
    "logit_bias       = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f90c6d5-9251-4322-bad6-469ac93e658c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2445: -100, 309: -100, 73: -100, 3200: -100}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the token IDs for the restricted words and apply a negative bias (-100)\n",
    "for word in restricted_words:\n",
    "    token_ids = enc.encode(word)\n",
    "    for token_id in token_ids:\n",
    "        logit_bias[token_id] = -100  # Heavily discourage these tokens\n",
    "\n",
    "logit_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23a271d9-ed5b-4b0d-9f10-d21c48791404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: Sure, here is an example of a poor and angry product review:\n",
      "\n",
      "\"I don't even know where to begin with how awful this product is. I wasted my hard-earned money on this piece of junk and it doesn't even work properly. The quality is terrible and it broke after just a few uses. I am absolutely livid with how terrible this product is. Do not waste your money on this garbage, it's a complete rip-off and the company should be ashamed of themselves for selling such\n"
     ]
    }
   ],
   "source": [
    "# Generate the response\n",
    "response_text = generate_story(prompt, logit_bias, apply_logit_bias='Y')\n",
    "print(\"Generated Response:\", response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b26db-5539-4e4f-a0d2-9f0202f7204d",
   "metadata": {},
   "source": [
    "#### Enhancing Brand or Tone Consistency\n",
    "**Tone Control:** If you need the model to consistently generate content that matches a specific brand voice, `logit_bias` can be used to prioritize specific words or phrases that align with that tone.  \n",
    "\n",
    "*Example:* For a luxury brand, you might boost words like \"premium,\" \"exclusive,\" or \"luxury\" to ensure the language feels high-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ea6d3-3165-42ba-b529-116ccdfbfd45",
   "metadata": {},
   "source": [
    "| **Without logit_bias**                        | **With logit_bias**                          |\n",
    "|-----------------------------------------------|---------------------------------------------|\n",
    "| \"Our products are good.\"                      | \"Our products are **premium**.\"            |\n",
    "| \"This service is fine.\"                       | \"This service is **exclusive**.\"           |\n",
    "| \"Our brand offers value.\"                     | \"Our brand offers **luxury**.\"             |\n",
    "| \"You will enjoy our offerings.\"               | \"You will enjoy our **high-end** offerings.\"|\n",
    "| \"Our team is helpful.\"                        | \"Our team is **dedicated to excellence**.\" |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f760a16a-0a08-4791-9956-07b61a5fdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the words you want to boost for a luxury tone\n",
    "boosted_words = [\"premium\", \"exclusive\", \"luxury\", \"excellence\"]\n",
    "logit_bias = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5c0ccda-94fe-405a-acd2-c7aa0382aec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{85694: 10, 90222: 10, 63959: 10, 3431: 10, 327: 10, 5997: 10, 768: 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the token IDs for the boosted words and apply a positive bias (100)\n",
    "for word in boosted_words:\n",
    "    token_ids = enc.encode(word)\n",
    "    for token_id in token_ids:\n",
    "        logit_bias[token_id] = 10  # Heavily encourage these tokens\n",
    "\n",
    "logit_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7339387b-c865-40c0-b602-832a70e91ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text \n",
    "def generate_brand_story(prompt, logit_bias, apply_logit_bias='N', max_tokens=100):\n",
    "    if apply_logit_bias.upper() == 'Y':\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "            messages=[\n",
    "                {\"role\": \"assistant\", \"content\": 'You are a luxury brand assistant'},\n",
    "                {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            logit_bias=logit_bias,\n",
    "            # stop       = [\"\\n\"]\n",
    "        )\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "            messages=[\n",
    "                {\"role\": \"assistant\", \"content\": 'You are a luxury brand assistant'},\n",
    "                {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            # logit_bias = logit_bias,\n",
    "            # stop       = [\"\\n\"]\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92a78905-fd1e-43ba-ab1e-67e9f38eed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt where the model might not use luxury-related terms\n",
    "prompt = \"Describe our new product offering and its features. Your responses should be in sentences format\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b511049-7f8d-4a4c-926a-ee23b19ea056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response Without logit_bias:\n",
      " Our new product offering is a premium leather handbag that combines elegance with functionality. It features high-quality, supple leather that is durable and luxurious to the touch. The handbag has multiple compartments and pockets to keep your belongings organized and easily accessible. It also comes with gold-tone hardware and a detachable shoulder strap for versatile carrying options. This handbag is perfect for the modern woman who values both style and practicality in her accessories.\n"
     ]
    }
   ],
   "source": [
    "# Generate the response without logit_bias\n",
    "response_text_without_bias = generate_brand_story(prompt=prompt, max_tokens= 2000, logit_bias=logit_bias, apply_logit_bias='N')\n",
    "print(\"Generated Response Without logit_bias:\\n\", response_text_without_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e20b3224-3d6e-4520-adb5-e30e0f1b651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response With logit_bias:\n",
      " Our new product offering is a limited edition handcrafted leather handbag. It is made from the finest Italian leather, meticulously designed with intricate embossing and gold-plated hardware for a luxurious finish. The bag features multiple compartments for organized storage and a detachable shoulder strap for versatile carrying options. With its timeless elegance and attention to detail, this handbag is a true showcase of craftsmanship and sophistication.\n"
     ]
    }
   ],
   "source": [
    "# Generate the response with logit_bias\n",
    "response_text_with_bias = generate_brand_story(prompt=prompt, max_tokens= 2000, logit_bias=logit_bias, apply_logit_bias='Y')\n",
    "print(\"Generated Response With logit_bias:\\n\", response_text_with_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70b8553a-1eb7-4a2d-973f-8f9ecc131efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Set the display options for full column width\n",
    "pd.set_option('display.max_colwidth', None)  # None means no limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b60f2beb-639f-42cf-8380-e7748091b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split sentences\n",
    "def split_sentences(text):\n",
    "    # Using regex to split sentences\n",
    "    return re.split(r'(?<=[.!?]) +', text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aad42d4d-4816-4110-bed9-53274dc2e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sentences from both responses\n",
    "without_bias_sentences = split_sentences(response_text_without_bias)\n",
    "with_bias_sentences    = split_sentences(response_text_with_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfb7cd08-9190-4736-ab5c-644bfb2801e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum length of the two lists\n",
    "max_length = max(len(without_bias_sentences), len(with_bias_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "441f176a-c8ba-4d66-99a3-fc121d0e7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the shorter list with empty strings\n",
    "without_bias_sentences += [\"\"] * (max_length - len(without_bias_sentences))\n",
    "with_bias_sentences    += [\"\"] * (max_length - len(with_bias_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30cd2d5e-2437-4027-8734-7202b390a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare the sentences\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Without Logit Bias\": without_bias_sentences,\n",
    "    \"With Logit Bias\": with_bias_sentences\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ec5e1f4-2fe2-4eda-a3b5-4c5b63defd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to highlight boosted words using HTML\n",
    "def highlight_boosted_words(text):\n",
    "    for word in boosted_words:\n",
    "        text = re.sub(rf'\\b({word})\\b', r'<span style=\"color:red; font-weight:bold;\">\\1</span>', text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f13a9d1c-bc08-43d6-8c25-f0682331942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply highlighting function to \"With Logit Bias\" column\n",
    "comparison_df[\"With Logit Bias\"] = comparison_df[\"With Logit Bias\"].apply(highlight_boosted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eedc7654-040f-4103-99a4-ff344c2b7997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a0323_row0_col0, #T_a0323_row0_col1, #T_a0323_row1_col0, #T_a0323_row1_col1, #T_a0323_row2_col0, #T_a0323_row2_col1, #T_a0323_row3_col0, #T_a0323_row3_col1, #T_a0323_row4_col0, #T_a0323_row4_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a0323\" style=\"width:100%\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a0323_level0_col0\" class=\"col_heading level0 col0\" >Without Logit Bias</th>\n",
       "      <th id=\"T_a0323_level0_col1\" class=\"col_heading level0 col1\" >With Logit Bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a0323_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a0323_row0_col0\" class=\"data row0 col0\" >Our new product offering is a premium leather handbag that combines elegance with functionality.</td>\n",
       "      <td id=\"T_a0323_row0_col1\" class=\"data row0 col1\" >Our new product offering is a limited edition handcrafted leather handbag.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0323_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a0323_row1_col0\" class=\"data row1 col0\" >It features high-quality, supple leather that is durable and luxurious to the touch.</td>\n",
       "      <td id=\"T_a0323_row1_col1\" class=\"data row1 col1\" >It is made from the finest Italian leather, meticulously designed with intricate embossing and gold-plated hardware for a luxurious finish.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0323_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a0323_row2_col0\" class=\"data row2 col0\" >The handbag has multiple compartments and pockets to keep your belongings organized and easily accessible.</td>\n",
       "      <td id=\"T_a0323_row2_col1\" class=\"data row2 col1\" >The bag features multiple compartments for organized storage and a detachable shoulder strap for versatile carrying options.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0323_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a0323_row3_col0\" class=\"data row3 col0\" >It also comes with gold-tone hardware and a detachable shoulder strap for versatile carrying options.</td>\n",
       "      <td id=\"T_a0323_row3_col1\" class=\"data row3 col1\" >With its timeless elegance and attention to detail, this handbag is a true showcase of craftsmanship and sophistication.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0323_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a0323_row4_col0\" class=\"data row4 col0\" >This handbag is perfect for the modern woman who values both style and practicality in her accessories.</td>\n",
       "      <td id=\"T_a0323_row4_col1\" class=\"data row4 col1\" ></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2cdef70b2c0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the DataFrame with HTML rendering\n",
    "comparison_df.style.set_table_attributes('style=\"width:100%\"').set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0490b1d0-458c-49a0-bc87-bf28ea51375e",
   "metadata": {},
   "source": [
    "#### Maths behind -  Logit Bias in Language Models\n",
    "\n",
    "The mathematics behind logit bias involves concepts from probability theory and statistics, particularly focusing on the softmax function, which converts raw scores (logits) into probabilities. \n",
    "\n",
    "Hereâ€™s a detailed breakdown of the mathematical principles involved:\n",
    "\n",
    "##### 1. Logits and Probability\n",
    "- **Logits**:\n",
    "    - In machine learning models, particularly in neural networks, the output of the model before applying any activation function is referred to as logits.\n",
    "    - These are real-valued numbers that can range from negative to positive infinity.\n",
    "  \n",
    "- **Softmax Function**: To convert logits into probabilities, the softmax function is applied:\n",
    " $$ \\Large\n",
    "  P(i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "  $$\n",
    "  \n",
    "  where:\n",
    "  - $P(i)$  is the probability of token $i$ .\n",
    "  - $z_i$  is the logit corresponding to token $i$ .\n",
    "  - The denominator sums the exponentials of all logits to normalize the probabilities across all tokens.\n",
    "\n",
    "##### 2. Effect of Logit Bias\n",
    "- When applying logit bias, the logits are adjusted before passing them into the softmax function. Letâ€™s consider how positive and negative biases impact the logits mathematically.\n",
    "\n",
    "- `Positive Bias`\n",
    "- **Adjustment**: When a positive bias \\( b \\) is applied to a token's logit \\( z_i \\), the new logit becomes:\n",
    "  $$\n",
    "  z'_i = z_i + b\n",
    "  $$\n",
    "  - If b = +100 , then $z'_i$ is significantly increased, making $ e^{z'_i} $ much larger compared to other logits. \n",
    "\n",
    "- `Negative Bias`\n",
    "- **Adjustment**: When a negative bias b is applied, the new logit becomes:\n",
    "  $$\n",
    "  z'_i = z_i + b\n",
    "  $$\n",
    "  - If b = -100 \\), then $z'_i $ is significantly decreased, making $ e^{z'_i}$ much smaller compared to other logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329170b5-e0e7-470f-abd1-495f2e70a943",
   "metadata": {},
   "source": [
    "#### Example Scenario\n",
    "Consider three tokens with their original logits:\n",
    "- $ z_1 = 2 $ (for token A)\n",
    "- $ z_2 = 1 $ (for token B)\n",
    "- $ z_3 = 0 $ (for token C)\n",
    "\n",
    "**Without Bias**:\n",
    "- The probabilities without any bias are calculated as:\n",
    "  $$\n",
    "  P(A) = \\frac{e^{2}}{e^{2} + e^{1} + e^{0}} = \\frac{e^{2}}{e^{2} + e + 1} \\approx 0.659\n",
    "  $$\n",
    "  $$\n",
    "  P(B) = \\frac{e^{1}}{e^{2} + e^{1} + e^{0}} \\approx 0.242\n",
    "  $$\n",
    "  $$\n",
    "  P(C) = \\frac{e^{0}}{e^{2} + e^{1} + e^{0}} \\approx 0.099\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1539c-4ce8-4d92-9143-325147c98c1c",
   "metadata": {},
   "source": [
    "**With Positive Bias (+100)**:\n",
    "- Adjusting token Aâ€™s logit with a bias of +100:\n",
    "  - $ z'_1 = 2 + 100 = 102 $\n",
    "- The probabilities become:\n",
    "  $$\n",
    "  P(A) = \\frac{e^{102}}{e^{102} + e^{1} + e^{0}} \\approx 1\n",
    "  $$\n",
    "  - The model will almost certainly choose token A due to the overwhelming increase in its logit.\n",
    "\n",
    "**With Negative Bias (-100)**:\n",
    "- Adjusting token Aâ€™s logit with a bias of -100:\n",
    "  - $ z'_1 = 2 - 100 = -98 $\n",
    "- The probabilities become:\n",
    "  $$\n",
    "  P(A) = \\frac{e^{-98}}{e^{-98} + e^{1} + e^{0}} \\approx 0\n",
    "  $$\n",
    "  - The model will almost never choose token A because its logit is so low compared to the others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422d22b-9efb-4142-b9b1-5c325722bdb4",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46a3fec5-5223-4250-b97c-983977d03e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example medical words you want to boost for urgency or emphasis\n",
    "boosted_words = [\"urgent\", \"emergency\", \"critical\", \"immediate\", \"life-threatening\", \"severe\"]\n",
    "logit_bias = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdfe3dd1-9157-42dc-ab0a-e42cca749f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{86153: 11,\n",
       " 99908: 11,\n",
       " 42641: 11,\n",
       " 318: 11,\n",
       " 14978: 11,\n",
       " 14789: 11,\n",
       " 62999: 11,\n",
       " 325: 11,\n",
       " 19846: 11}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the token IDs for the boosted words and apply a positive bias (100)\n",
    "for word in boosted_words:\n",
    "    token_ids = enc.encode(word)\n",
    "    for token_id in token_ids:\n",
    "        logit_bias[token_id] = 11  # Heavily encourage these tokens\n",
    "\n",
    "logit_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd9b9f3e-b22c-48d4-9d5a-55a39ae10b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate medical report with logit bias applied\n",
    "def generate_medical_report(prompt, logit_bias, apply_logit_bias='N', max_tokens=100):\n",
    "    if apply_logit_bias.upper() == 'Y':\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Adjust model as necessary\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical assistant generating urgent patient reports.\"},\n",
    "                {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            logit_bias=logit_bias\n",
    "        )\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Adjust model as necessary\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical assistant generating patient reports.\"},\n",
    "                {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "            ],\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "049b13db-341a-47b8-a6ee-35a12ceae75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Medical Report With logit_bias:\n",
      " The patient presented to the clinic with complaints of chest pain and shortness of breath. On examination, vital signs revealed tachycardia and tachypnea. Auscultation of the chest revealed decreased breath sounds on the right side with crackles present. An electrocardiogram showed signs of myocardial ischemia. Blood tests revealed elevated cardiac enzymes, indicating a possible myocardial infarction. The patient was immediately referred to the emergency department for further evaluation and management.\n",
      "\n",
      "In summary, the patient's presentation of chest pain, shortness of breath, and abnormal vital signs, along with the findings on physical examination and diagnostic tests, are consistent with a suspected acute myocardial infarction. Timely intervention is crucial to prevent further complications and improve the patient's outcomes. The healthcare team is coordinating closely to ensure prompt and appropriate treatment for the patient.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt for generating a medical report\n",
    "prompt = \"Generate a medical summary report in 2 paragraphs for a patient with chest pain and shortness of breath.\"\n",
    "\n",
    "# Generate the response with logit bias applied for urgent medical terms\n",
    "response_text_with_bias = generate_medical_report(prompt=prompt, max_tokens=200, logit_bias=logit_bias, apply_logit_bias='Y')\n",
    "print(\"Generated Medical Report With logit_bias:\\n\", response_text_with_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a3642-14d5-42f7-9e81-8a6a63c2dddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
