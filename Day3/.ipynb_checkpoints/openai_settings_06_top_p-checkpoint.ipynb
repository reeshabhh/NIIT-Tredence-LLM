{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47d5470-7d41-48bb-b23e-f0e66aa3bdac",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### **Top-p Sampling in OpenAI GPT**\n",
    "----------------------------------\n",
    "\n",
    "**Description:**\n",
    "- Top-p sampling, also known as \"nucleus sampling,\" is a decoding technique used during text generation.\n",
    "- Instead of considering all possible tokens during generation (as in greedy decoding), top-p sampling focuses on a subset of tokens whose cumulative probability is above a specified threshold \\( p \\).\n",
    "- This allows the model to dynamically adjust the number of tokens it considers at each step based on the probability distribution of the tokens.\n",
    "\n",
    "- **top_p = 1**: This includes all tokens in the probability distribution (equivalent to no restriction).\n",
    "- **top_p < 1**: The model samples from a smaller set of high-probability tokens, adding randomness while still maintaining some control over the coherence of the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725580c-d970-4617-84dd-7f95c1598690",
   "metadata": {},
   "source": [
    "**Example Illustration:**\n",
    "- If `top_p = 0.9`, the model will consider the smallest subset of tokens whose cumulative probability is at least 90%. The remaining tokens are ignored, preventing low-probability (often irrelevant) tokens from being selected.\n",
    "  \n",
    "For instance, if a model predicts the following probabilities for the next word:\n",
    "- \"dog\": 0.4\n",
    "- \"cat\": 0.3\n",
    "- \"mouse\": 0.2\n",
    "- \"elephant\": 0.05\n",
    "- \"lion\": 0.05\n",
    "\n",
    "With **top_p = 0.9**, only the first three words (\"dog\", \"cat\", \"mouse\") will be considered, as their cumulative probability is 0.9. The tokens \"elephant\" and \"lion\" will be excluded from sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed83579-afae-4408-aa08-2434d74e4808",
   "metadata": {},
   "source": [
    "#### Greedy Decoding:\n",
    "- **How it works:** Selects the token with the highest probability at each step.\n",
    "- **Result:** Deterministic and often repetitive output.\n",
    "- **Use case:** Best for short, factual responses where randomness is not desired.\n",
    "\n",
    "#### Top-p Sampling:\n",
    "- **How it works:** Selects a token from the smallest subset of tokens whose cumulative probability meets or exceeds the threshold `p`. Randomly samples from this subset.\n",
    "- **Result:** More creative and varied output, with controlled randomness.\n",
    "- **Use case:** Ideal for tasks that benefit from creativity or diverse responses, like story generation or dialogue systems.\n",
    "\n",
    "#### Key Difference:\n",
    "- Greedy decoding picks the single highest-probability token every time.\n",
    "- Top-p sampling considers a range of tokens and samples from them, depending on the set defined by `p`, leading to more diverse outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a240074-779e-4159-81e4-65d95919a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765b0348-682f-4160-8d48-3dfaf4722acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7f8bfb6-929c-4e75-b3df-e0ceede17ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # api_key = openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06d9f57-e6e3-4a35-b8f9-d45baf15e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Once upon a time, in a distant land, there was a kingdom ruled by a wise and just king who\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae123ac9-dbb1-4569-8d35-3d84e1d13281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was loved by all his subjects. The kingdom was prosperous and peaceful, and the people lived happily under the king's rule.\n",
      "\n",
      "One day, a dark cloud appeared on the horizon. A neighboring kingdom, ruled by a cruel and power-hungry queen, declared war on the peaceful kingdom. The queen's army was vast and powerful, and it seemed like the peaceful kingdom stood no chance against their might.\n",
      "\n",
      "The king knew that he had to protect his people and his kingdom at all costs. He called upon his bravest knights and soldiers to defend the kingdom against the invading army. The people of the kingdom rallied behind their king, ready to fight for their home and their freedom.\n",
      "\n",
      "The battle was fierce and bloody, but the king's army fought with\n"
     ]
    }
   ],
   "source": [
    "# top_p decoding with temperature 0\n",
    "response_greedy = client.chat.completions.create(\n",
    "        model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "        messages   = [\n",
    "            {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "        ],\n",
    "        max_tokens = 150,\n",
    "        top_p      = 0.15,  # Top-p sampling for creative diversity\n",
    "        temperature= 0.0    # Introduce a bit of randomness\n",
    "    )\n",
    "\n",
    "print(response_greedy.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ad929-f6d7-46ac-b640-fc89434e09c2",
   "metadata": {},
   "source": [
    "#### Key Differences in How `temperature` and `top_p` Influence Randomness\n",
    "\n",
    "##### Temperature alone can introduce randomness:\n",
    "- **What it does**: It scales the entire probability distribution. Lowering `temperature` makes the model more deterministic (similar to greedy decoding), while raising it allows for more exploration of less probable tokens.\n",
    "- **Effect**: It introduces **randomness globally** across all tokens.\n",
    "\n",
    "##### Top-p alone can influence randomness, but in a more controlled way:\n",
    "- **What it does**: It restricts the model to choosing from a subset of the most probable tokens whose cumulative probability is within the threshold `p`. It still randomly samples from this subset.\n",
    "- **Effect**: It introduces **randomness locally** within the top probable tokens but prevents the model from choosing extremely low-probability tokens.\n",
    "\n",
    "---\n",
    "\n",
    "##### Why `top_p` Works Without Temperature:\n",
    "Even without setting `temperature`, **top-p sampling** (or nucleus sampling) introduces randomness by restricting the token pool, ensuring the model samples from the top `p%` of probable tokens. \n",
    "\n",
    "##### Example:\n",
    "- If you set `top_p = 0.9`, the model will sample tokens only from the subset where their cumulative probability is 90%. \n",
    "- This **limits randomness** to the most probable tokens but still samples randomly from this subset.\n",
    "- **Temperature is not required** for top-p to work because it inherently involves randomness in selecting among the probable tokens.\n",
    "\n",
    "---\n",
    "\n",
    "##### When Does `top_p` Need Temperature?\n",
    "\n",
    "- **top_p** restricts the token pool to the most probable tokens.\n",
    "- **temperature** controls the level of randomness in choosing from that pool.\n",
    "\n",
    "When you combine both:\n",
    "- `top_p` defines the **subset** of most probable tokens.\n",
    "- `temperature` **smooths the probabilities** within the restricted token pool, allowing a gradient of selection, rather than sharp deterministic selections.\n",
    "\n",
    "By setting both, you gain **finer control** over the randomness in the generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb25f1e-09e9-4777-bce6-4ca9a4d4961d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
